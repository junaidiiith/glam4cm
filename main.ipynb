{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "datasets_dir = 'datasets'\n",
    "ecore_json_path = os.path.join(datasets_dir, 'ecore_555/ecore_555.jsonl')\n",
    "mar_json_path = os.path.join(datasets_dir, 'mar-ecore-github/ecore-github.jsonl')\n",
    "modelsets_uml_json_path = os.path.join(datasets_dir, 'modelset/uml.jsonl')\n",
    "modelsets_ecore_json_path = os.path.join(datasets_dir, 'modelset/ecore.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ecore_555 from pickle\n",
      "Loaded ecore_555 from pickle\n",
      "Graphs: 548\n",
      "Loading modelset from pickle\n",
      "Loaded modelset from pickle\n",
      "Graphs: 2043\n",
      "Loading mar-ecore-github from pickle\n",
      "Loaded mar-ecore-github from pickle\n",
      "Graphs: 18110\n"
     ]
    }
   ],
   "source": [
    "from data_loading.dataset import ModelDataset\n",
    "\n",
    "\n",
    "reload = False\n",
    "ecore = ModelDataset('ecore_555', reload=reload)\n",
    "modelset = ModelDataset('modelset', reload=reload, remove_duplicates=True)\n",
    "mar = ModelDataset('mar-ecore-github', reload=reload)\n",
    "\n",
    "\n",
    "datasets = {\n",
    "    'ecore': ecore,\n",
    "    'modelset': modelset,\n",
    "    'mar': mar\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Article | BIBTEX Entry | Chapter Booklet fields AbstractField Year Authors AuthorUrls Isbn Issn Institution Publisher School Howpublished Url key LocatedElement Day Type Inproceedings Manual BookTitle Organization Editor Field Series Doi Text Bibtex Edition Book Month Pages Title Number MastersThesis Note Techreport Misc Volume Address Proceedings Inbook Journal Incollection PhdThesis'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecore[0].find_node_str_upto_distance(8, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fasttext classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "from encoding.common import doc_tokenizer\n",
    "from encoding.encoders import (\n",
    "    BertTokenizerEncoder,\n",
    "    FasttextEncoder,\n",
    "    ClassLabelEncoder,\n",
    "    TFIDFEncoder,\n",
    "    BertTFIDF\n",
    ")\n",
    "\n",
    "for name, dataset in datasets.items():\n",
    "    if name not in ['ecore', 'modelset']:\n",
    "        continue\n",
    "    print(\"Dataset: \", name)\n",
    "    i = 0\n",
    "    accuracies, bal_accuracies = [], []\n",
    "    for X_train, X_test, y_train, y_test in dataset.k_fold_split():\n",
    "        print(\"Fold number: \", i+1)\n",
    "        f_train = f'datasets/fasttext_train_{name}_{i}.txt'\n",
    "        f_test = f'datasets/fasttext_test_{name}_{i}.txt'\n",
    "        if not os.path.exists(f_train):\n",
    "            with open(f_train, 'w') as f:\n",
    "                for x, y in zip(X_train, y_train):\n",
    "                    x = \" \".join(doc_tokenizer(x))\n",
    "                    f.write(f\"__label__{y} {x}\\n\")\n",
    "        \n",
    "        if not os.path.exists(f_test):\n",
    "            with open(f_test, 'w') as f:\n",
    "                for x, y in zip(X_test, y_test):\n",
    "                    x = \" \".join(doc_tokenizer(x))\n",
    "                    f.write(f\"__label__{y} {x}\\n\")\n",
    "        \n",
    "        if os.path.exists(f'models/{name}_{i}.bin'):\n",
    "            model = fasttext.load_model(f'models/{name}_{i}.bin')\n",
    "        else:\n",
    "            model = fasttext.train_supervised(\n",
    "                input=f_train, \n",
    "                epoch=100, \n",
    "                lr=0.2, \n",
    "                wordNgrams=2, \n",
    "            )\n",
    "            model.save_model(f'models/{name}_{i}.bin')        \n",
    "        y_pred = model.predict([i.strip() for i in open(f_test).readlines()])[0]\n",
    "        y_true = [i.split()[0].split('__label__')[1] for i in open(f_test).readlines()]\n",
    "        y_pred = [i[0].split('__label__')[1] for i in y_pred]\n",
    "\n",
    "\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        bal_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "        print(f\"Accuracy: {accuracy}, Balanced Accuracy: {bal_accuracy}\")\n",
    "        accuracies.append(accuracy)\n",
    "        bal_accuracies.append(bal_accuracy)\n",
    "\n",
    "        i += 1            \n",
    "    print(f\"Average Accuracy: {np.mean(accuracies)}, Average Balanced Accuracy: {np.mean(bal_accuracies)}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fasttext word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_udata = list(set([g.text for dataset in datasets.values() for g in dataset]))\n",
    "X_udata = [f\" \".join(doc_tokenizer(x)) for x in X_udata]\n",
    "f_udata = 'datasets/fasttext_udata.txt'\n",
    "with open(f'{f_udata}', 'w') as f:\n",
    "    for x in X_udata:\n",
    "        f.write(f\"{x}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  8120\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:    7026 lr: -0.000001 avg.loss:  1.111646 ETA:   0h 0m 0s 60.3% words/sec/thread:    7034 lr:  0.039708 avg.loss:  1.177690 ETA:   0h 3m12s100.0% words/sec/thread:    7026 lr:  0.000000 avg.loss:  1.111496 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.train_unsupervised(\n",
    "    input=f_udata, \n",
    "    epoch=500, \n",
    "    lr=0.1,\n",
    "    minn=2,\n",
    "    maxn=5,\n",
    "    dim=128\n",
    ")\n",
    "model.save_model(\"models/uml_fasttext.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_encoder = TFIDFEncoder()\n",
    "bert_encoder = BertTokenizerEncoder('bert-base-uncased')\n",
    "bert_tfidf_encoder = BertTFIDF('bert-base-uncased')\n",
    "fasttext_encoder = FasttextEncoder('models/uml_fasttext.bin')\n",
    "class_label_encoder = ClassLabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    balanced_accuracy_score\n",
    ")\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "def train_svm(dataset: ModelDataset, encoder: Union[TFIDFEncoder, BertTFIDF, FasttextEncoder]):\n",
    "    accuracies, bal_accuracies = [], []\n",
    "    for train_idx, test_idx in dataset.k_fold_split():\n",
    "        X = encoder.encode(dataset.data[0])\n",
    "        y = class_label_encoder.encode(dataset.data[1])\n",
    "\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        svm_classifier = svm.SVC(kernel='linear')  # You can change the kernel as needed\n",
    "        svm_classifier.fit(X_train, y_train)\n",
    "        # Predict on the test set\n",
    "        y_pred = svm_classifier.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        # print(f'SVM Classifier Accuracy: {accuracy}')\n",
    "        bal_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "        # print(f'SVM Classifier Balanced Accuracy: {bal_accuracy}')\n",
    "\n",
    "        accuracies.append(accuracy)\n",
    "        bal_accuracies.append(bal_accuracy)\n",
    "    \n",
    "    print(f'Mean Accuracy: {np.mean(accuracies)}')\n",
    "    print(f'Mean Balanced Accuracy: {np.mean(bal_accuracies)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_svm(modelset, tf_idf_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.5969380140304565, 'petrinetv3'),\n",
       " (0.5963557362556458, 'petrinetv1'),\n",
       " (0.5946762561798096, 'petrinetv2'),\n",
       " (0.5399251580238342, 'petri'),\n",
       " (0.5047121047973633, 'tokens')]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_nearest_neighbors('petrinet', k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def split_into_chunks(text, max_length=512):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = [tokens[i:i + max_length] for i in range(0, len(tokens), max_length)]\n",
    "    return [' '.join(chunk) for chunk in chunks]\n",
    "\n",
    "# # Example usage\n",
    "long_text = max(modelset, key=lambda x: len(x.text)).text\n",
    "chunks = split_into_chunks(long_text)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    Trainer, \n",
    "    TrainingArguments\n",
    ")\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, \n",
    "    AutoTokenizer\n",
    ")\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from data_loading.dataset import Dataset\n",
    "from settings import device, seed\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from trainers.metrics import compute_metrics\n",
    "\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "max_length_map = {\n",
    "    'bert-base-uncased': 512,\n",
    "    'allenai/longformer-base-4096': 4096\n",
    "}\n",
    "\n",
    "\n",
    "# Create your dataset\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.inputs = tokenizer(\n",
    "            texts, \n",
    "            return_tensors='pt', \n",
    "            truncation=True, \n",
    "            padding='max_length', \n",
    "            max_length=max_length\n",
    "        )\n",
    "        self.inputs['labels'] = torch.tensor(labels, dtype=torch.long)\n",
    " \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs['input_ids'])\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = {key: val[index] for key, val in self.inputs.items()}\n",
    "        return item\n",
    "\n",
    "\n",
    "def train_hf(model_name, model_ds: Dataset, epochs):\n",
    "    max_len = max_length_map[model_name]\n",
    "    i = 0\n",
    "    print(f'Device used: {device}')\n",
    "\n",
    "    for train_idx, test_idx in model_ds.k_fold_split():\n",
    "        print(f'Fold number: {i+1}')\n",
    "        X, y = model_ds.data\n",
    "        print(f'X: {len(X)}, y: {len(y)}')\n",
    "        y = LabelEncoder().fit_transform(y)\n",
    "        X_train, X_test = [X[i] for i in train_idx], [X[i] for i in test_idx]\n",
    "        y_train, y_test = [y[i] for i in train_idx], [y[i] for i in test_idx]\n",
    "\n",
    "        print(f'Train: {len(X_train)}, Test: {len(X_test)}')\n",
    "\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(set(y)))\n",
    "        model.to(device)\n",
    "\n",
    "        train_ds = CustomDataset(X_train, y_train, tokenizer, max_length=max_len)\n",
    "        test_ds = CustomDataset(X_test, y_test, tokenizer, max_length=max_len)\n",
    "\n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir='./results',\n",
    "            num_train_epochs=epochs,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            per_device_train_batch_size=2,\n",
    "            per_device_eval_batch_size=2,\n",
    "            warmup_steps=500,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir='./logs',\n",
    "            logging_steps=10,\n",
    "            load_best_model_at_end=True,\n",
    "            save_total_limit=1,\n",
    "            fp16=True,\n",
    "            seed=42\n",
    "        )\n",
    "\n",
    "        # Trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_ds,\n",
    "            eval_dataset=test_ds,\n",
    "            compute_metrics=compute_metrics            \n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "        results = trainer.evaluate()\n",
    "        print(results)\n",
    "\n",
    "        i += 1\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hf('bert-base-uncased', modelset, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold number: 1\n",
      "X: 2043, y: 2043\n",
      "Train: 1838, Test: 205\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from settings import device\n",
    "from data_loading.dataset import EncodingDataset\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "max_len = 512\n",
    "\n",
    "i = 0\n",
    "for train_idx, test_idx in modelset.k_fold_split():\n",
    "    print(f'Fold number: {i+1}')\n",
    "    X, y = modelset.data\n",
    "    print(f'X: {len(X)}, y: {len(y)}')\n",
    "    y = LabelEncoder().fit_transform(y)\n",
    "    X_train, X_test = [X[i] for i in train_idx], [X[i] for i in test_idx]\n",
    "    y_train, y_test = [y[i] for i in train_idx], [y[i] for i in test_idx]\n",
    "\n",
    "    print(f'Train: {len(X_train)}, Test: {len(X_test)}')\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('results/checkpoint-1380', num_labels=len(set(y)))\n",
    "    model.to(device)\n",
    "\n",
    "    train_ds = EncodingDataset(tokenizer, X_train, y_train, max_length=max_len)\n",
    "    test_ds = EncodingDataset(tokenizer, X_test, y_test, max_length=max_len)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([205, 512])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds[:]['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    #### Put vaues of custom data on device\n",
    "\n",
    "    test_ds = {k: v.to(device) for k, v in test_ds[:].items()}\n",
    "\n",
    "    outputs = model(**test_ds)\n",
    "    pred_classes = torch.argmax(outputs.logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 27, 10]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = pred_classes.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embeddings.bert import BertEmbedder\n",
    "\n",
    "ft_embedder = BertEmbedder('bert-base-uncased', 'results/checkpoint-1380')\n",
    "bert_embedder = BertEmbedder('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 27)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = modelset[0].get_node_texts()\n",
    "len(texts), modelset[0].number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d503e56f6a4336bc2190bc78f7d1d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing modelset:   0%|          | 0/2043 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fed588286e048719e84ee2f9f2bea29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing modelset:   0%|          | 0/2043 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from data_loading.dataset import GraphDataset\n",
    "\n",
    "graph_dataset_ft = GraphDataset(modelset, ft_embedder)\n",
    "graph_dataset = GraphDataset(modelset, bert_embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.gnn_layers import GNNClassifier\n",
    "\n",
    "graph_classifier = GNNClassifier(\n",
    "    gnn_conv_model='SAGEConv',\n",
    "    input_dim=graph_dataset_ft.num_features,\n",
    "    hidden_dim=64,\n",
    "    output_dim=graph_dataset_ft.num_classes,\n",
    "    num_layers=2,\n",
    "    num_heads=None,\n",
    "    dropout=0.1,\n",
    "    residual=False,\n",
    "    pool='sum',\n",
    "    use_appnp=True,\n",
    "    K=10,\n",
    "    alpha=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainers.graph_classifier import GNNTrainer\n",
    "gnn_trainer = GNNTrainer(\n",
    "    graph_classifier,\n",
    "    graph_dataset_ft,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "mapping = json.load(open('datasets/graph_data/modelset/mapping.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in gnn_trainer.dataloaders['train']:\n",
    "    loss = gnn_trainer.step(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[288, 768], edge_index=[2, 560], y=[16], batch=[288], ptr=[17])\n",
      "Batch x shape: torch.Size([288, 768])\n",
      "Batch edge_index shape: torch.Size([2, 560])\n",
      "Batch y shape: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Example graph dataset\n",
    "class ExampleGraphDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, num_graphs):\n",
    "        self.graphs = []\n",
    "        for i in range(num_graphs):\n",
    "            # Create dummy data for the example\n",
    "            num_nodes = 18\n",
    "            num_node_features = 768\n",
    "            num_edges = 35\n",
    "            num_classes = 12\n",
    "\n",
    "            x = torch.randn(num_nodes, num_node_features)  # Node features\n",
    "            edge_index = torch.randint(0, num_nodes, (2, num_edges), dtype=torch.long)  # Random edge indices\n",
    "            y = torch.tensor([i % num_classes], dtype=torch.long)  # Graph label\n",
    "            self.graphs.append(Data(x=x, edge_index=edge_index, y=y))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.graphs[idx]\n",
    "\n",
    "# Instantiate the dataset\n",
    "dataset = ExampleGraphDataset(num_graphs=100)\n",
    "\n",
    "# Create the DataLoader for batching\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Iterating over the DataLoader\n",
    "for batch in loader:\n",
    "    print(batch)\n",
    "    print(f'Batch x shape: {batch.x.shape}')\n",
    "    print(f'Batch edge_index shape: {batch.edge_index.shape}')\n",
    "    print(f'Batch y shape: {batch.y.shape}')\n",
    "    break  # Remove this break to iterate over all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import (\n",
    "    negative_sampling, \n",
    "    train_test_split_edges\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Positive Graph: Data(x=[18, 768], edge_index=[2, 16])\n",
      "Train Negative Graph: Data(x=[18, 768], edge_index=[2, 16])\n",
      "Test Positive Graph: Data(x=[18, 768], edge_index=[2, 16])\n",
      "Test Negative Graph: Data(x=[18, 768], edge_index=[2, 16])\n",
      "Train Graph with Masked Edges: Data(x=[18, 768], edge_index=[2, 16], edge_label=[8], edge_label_index=[2, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "def get_pos_neg_graphs(X, E, test_ratio=0.2):\n",
    "    # Create a Data object\n",
    "    data = Data(x=X, edge_index=E)\n",
    "\n",
    "    # Apply RandomLinkSplit\n",
    "    transform = T.RandomLinkSplit(\n",
    "        num_val=0, \n",
    "        num_test=test_ratio, \n",
    "        is_undirected=True, \n",
    "        add_negative_train_samples=False\n",
    "    )\n",
    "    train_data, _, test_data = transform(data)\n",
    "\n",
    "    # Positive edges\n",
    "    train_pos_edge_index = train_data.edge_index\n",
    "    test_pos_edge_index = test_data.edge_index\n",
    "\n",
    "    # Negative edges\n",
    "    train_neg_edge_index = negative_sampling(\n",
    "        edge_index=train_pos_edge_index, \n",
    "        num_nodes=data.num_nodes, \n",
    "        num_neg_samples=train_pos_edge_index.size(1)\n",
    "    )\n",
    "\n",
    "    test_neg_edge_index = negative_sampling(\n",
    "        edge_index=test_pos_edge_index, \n",
    "        num_nodes=data.num_nodes, \n",
    "        num_neg_samples=test_pos_edge_index.size(1)\n",
    "    )\n",
    "\n",
    "    # Create the graph objects\n",
    "    train_pos_g = Data(x=X, edge_index=train_pos_edge_index)\n",
    "    train_neg_g = Data(x=X, edge_index=train_neg_edge_index)\n",
    "    test_pos_g = Data(x=X, edge_index=test_pos_edge_index)\n",
    "    test_neg_g = Data(x=X, edge_index=test_neg_edge_index)\n",
    "\n",
    "    graphs = {\n",
    "        'train_pos_g': train_pos_g,\n",
    "        'train_neg_g': train_neg_g,\n",
    "        'test_pos_g': test_pos_g,\n",
    "        'test_neg_g': test_neg_g,\n",
    "        'train_g': train_data\n",
    "    }\n",
    "    return graphs\n",
    "\n",
    "# Example usage:\n",
    "num_nodes = 18\n",
    "num_node_features = 768\n",
    "num_edges = 35\n",
    "\n",
    "X = torch.randn(num_nodes, num_node_features)  # Node features\n",
    "E = torch.randint(0, num_nodes, (2, num_edges), dtype=torch.long)  # Random edge indices\n",
    "\n",
    "graphs = get_pos_neg_graphs(X, E, 0.6)\n",
    "\n",
    "print(\"Train Positive Graph:\", graphs['train_pos_g'])\n",
    "print(\"Train Negative Graph:\", graphs['train_neg_g'])\n",
    "print(\"Test Positive Graph:\", graphs['test_pos_g'])\n",
    "print(\"Test Negative Graph:\", graphs['test_neg_g'])\n",
    "print(\"Train Graph with Masked Edges:\", graphs['train_g'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[18, 768], edge_index=[2, 16], edge_label=[8], edge_label_index=[2, 8])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphs['train_g']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
