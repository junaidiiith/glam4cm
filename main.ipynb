{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "datasets_dir = 'datasets'\n",
    "ecore_json_path = os.path.join(datasets_dir, 'ecore_555/ecore_555.jsonl')\n",
    "mar_json_path = os.path.join(datasets_dir, 'mar-ecore-github/ecore-github.jsonl')\n",
    "modelsets_uml_json_path = os.path.join(datasets_dir, 'modelset/uml.jsonl')\n",
    "modelsets_ecore_json_path = os.path.join(datasets_dir, 'modelset/ecore.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import fasttext\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from re import finditer\n",
    "\n",
    "\n",
    "SEP = ' '\n",
    "def camel_case_split(identifier):\n",
    "    matches = finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
    "    return [m.group(0) for m in matches]\n",
    "\n",
    "\n",
    "def doc_tokenizer(doc):\n",
    "    words = doc.split()\n",
    "    # split _\n",
    "    words = [w2 for w1 in words for w2 in w1.split('_') if w2 != '']\n",
    "    # camelcase\n",
    "    words = [w2.lower() for w1 in words for w2 in camel_case_split(w1) if w2 != '']\n",
    "    return words\n",
    "\n",
    "\n",
    "class TFIDFEncoder:\n",
    "    def __init__(self, X=None):\n",
    "        self.encoder = TfidfVectorizer(\n",
    "            lowercase=False, tokenizer=doc_tokenizer, min_df=3\n",
    "        )\n",
    "\n",
    "        if X:\n",
    "            self.encode(X)\n",
    "\n",
    "    def encode(self, X):\n",
    "        # print('Fitting TFIDF')\n",
    "        X_t = self.encoder.fit_transform(X)\n",
    "        X_sp = csr_matrix(np.vstack([x.toarray() for x in X_t]))\n",
    "        # print('TFIDF Encoded')\n",
    "        return X_sp\n",
    "\n",
    "\n",
    "class BertTokenizerEncoder:\n",
    "    def __init__(self, name, X=None):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "\n",
    "        if X:\n",
    "            self.encode(X)\n",
    "        \n",
    "\n",
    "    def encode(self, X, batch_encode=False, percentile=100):\n",
    "        # print('Tokenizing Bert')\n",
    "        tokens = self.tokenizer(X)\n",
    "\n",
    "        if batch_encode:\n",
    "            lengths = [len(i) for i in tokens['input_ids']]\n",
    "            size = int(np.percentile(lengths, percentile)) if percentile < 100 else max(lengths)\n",
    "            if size > 512:\n",
    "                print(f'WARNING: Max size is {size}. Truncating to 512')\n",
    "            size = max(size, 512)\n",
    "            \n",
    "            tokenized_data = self.tokenizer(\n",
    "                X, \n",
    "                padding=True, \n",
    "                truncation=True, \n",
    "                max_length=size\n",
    "            )\n",
    "        else:\n",
    "            tokenized_data = self.tokenizer(X)\n",
    "        # print('Bert Tokenized')\n",
    "\n",
    "        return tokenized_data\n",
    "\n",
    "\n",
    "class BertTFIDF:\n",
    "    def __init__(self, name, X=None):\n",
    "        self.bert = BertTokenizerEncoder(name)\n",
    "        self.tfidf = TFIDFEncoder()\n",
    "\n",
    "        if X:\n",
    "            self.encode(X)\n",
    "\n",
    "    def encode(self, X):\n",
    "        X_b = [f\"{SEP}\".join([str(j) for j in i]) for i in self.bert.encode(X)['input_ids']]\n",
    "        X_t = self.tfidf.encode(X_b)\n",
    "        return X_t\n",
    "\n",
    "\n",
    "class FasttextEncoder:\n",
    "    def __init__(self, model_name, X=None):\n",
    "        self.model = fasttext.load_model(model_name)\n",
    "        if X:\n",
    "            self.encode(X)\n",
    "\n",
    "    def encode(self, X):\n",
    "        def get_sentence_embedding(sentence):\n",
    "            return self.model.get_sentence_vector(sentence)\n",
    "        \n",
    "        # print('Encoding Fasttext')\n",
    "        X_t = [\" \".join(doc_tokenizer(i)) for i in X]\n",
    "        X_t = np.array([get_sentence_embedding(i) for i in X_t])\n",
    "        # print('Fasttext Encoded')\n",
    "        return X_t\n",
    "\n",
    "\n",
    "class ClassLabelEncoder(LabelEncoder):\n",
    "    def __init__(self, y=None) -> None:\n",
    "        super().__init__()\n",
    "        if y:\n",
    "            self.fit(y)\n",
    "    \n",
    "    def encode(self, y):\n",
    "        return self.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ecore_555 from pickle\n",
      "Loaded ecore_555 from pickle\n",
      "Graphs: 548\n",
      "Loading modelset from pickle\n",
      "Loaded modelset from pickle\n",
      "Graphs: 2043\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Union\n",
    "import networkx as nx\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "from random import shuffle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "class GenericGraph(nx.DiGraph):\n",
    "    def __init__(self, json_obj: dict, use_type=False):\n",
    "        super().__init__()\n",
    "        self.use_type = use_type\n",
    "        self.json_obj = json_obj\n",
    "        self.graph_id = json_obj.get('ids')\n",
    "        self.graph_type = json_obj.get('model_type')\n",
    "        self.label = json_obj.get('labels')\n",
    "        self.is_duplicated = json_obj.get('is_duplicated')\n",
    "        self.directed = json.loads(json_obj.get('graph')).get('directed')\n",
    "        self.create_graph(json_obj)\n",
    "        self.text = json_obj.get('txt')\n",
    "\n",
    "\n",
    "    def create_graph(self, json_obj):\n",
    "        graph = json.loads(json_obj['graph'])\n",
    "        nodes = graph['nodes']\n",
    "        edges = graph['links']\n",
    "        for node in nodes:\n",
    "            self.add_node(node['id'], **node)\n",
    "        for edge in edges:\n",
    "            self.add_edge(edge['source'], edge['target'], **edge)\n",
    "    \n",
    "    # @property\n",
    "    # def text(self):\n",
    "    #     txt = list()\n",
    "    #     for _, d in self.nodes(data=True):\n",
    "    #         etype = d.get('type', '')\n",
    "    #         name = d.get('name', '')\n",
    "    #         node_data = f\"{name}{etype if self.use_type else ''}\"\n",
    "    #         txt.append(node_data)\n",
    "    #     return SEP.join(txt).strip()\n",
    "\n",
    "\n",
    "    def get_node_embeddings(self):\n",
    "        pass\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'{self.json_obj}\\nGraph({self.graph_id}, nodes={self.number_of_nodes()}, edges={self.number_of_edges()})'\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(\n",
    "            self, \n",
    "            dataset_name: str, \n",
    "            dataset_dir = datasets_dir,\n",
    "            save_dir = 'datasets/pickles',\n",
    "            reload=False,\n",
    "            remove_duplicates=False,\n",
    "            extension='.jsonl'\n",
    "        ):\n",
    "        self.name = dataset_name\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.save_dir = save_dir\n",
    "        self.extension = extension\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        dataset_exists = os.path.exists(os.path.join(save_dir, f'{dataset_name}.pkl'))\n",
    "        if reload or not dataset_exists:\n",
    "            self.graphs: List[GenericGraph] = []\n",
    "            data_path = os.path.join(dataset_dir, dataset_name)\n",
    "            for file in os.listdir(data_path):\n",
    "                if file.endswith(self.extension) and file.startswith('ecore'):\n",
    "                    json_objects = json.load(open(os.path.join(data_path, file)))\n",
    "                    self.graphs += [\n",
    "                        GenericGraph(g) for g in tqdm(\n",
    "                            json_objects, desc=f'Loading {dataset_name.title()}'\n",
    "                        )\n",
    "                    ]\n",
    "            self.save()\n",
    "        \n",
    "        else:\n",
    "            self.load()\n",
    "        \n",
    "        if remove_duplicates:\n",
    "            self.remove_duplicates()\n",
    "\n",
    "        print(f'Graphs: {len(self.graphs)}')\n",
    "\n",
    "\n",
    "    def remove_duplicates(self):\n",
    "        self.graphs = self.dedup()\n",
    "\n",
    "    def dedup(self) -> List[GenericGraph]:\n",
    "        return [g for g in self.graphs if not g.is_duplicated]\n",
    "    \n",
    "    \n",
    "    def get_train_test_split(self, train_size=0.8):\n",
    "        n = len(self.graphs)\n",
    "        train_size = int(n * train_size)\n",
    "        idx = list(range(n))\n",
    "        shuffle(idx)\n",
    "        train_idx = idx[:train_size]\n",
    "        test_idx = idx[train_size:]\n",
    "        return train_idx, test_idx\n",
    "    \n",
    "\n",
    "    def k_fold_split(\n",
    "            self,  \n",
    "            k=10\n",
    "        ):\n",
    "        kfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "        n = len(self.graphs)\n",
    "        for train_idx, test_idx in kfold.split(np.zeros(n), np.zeros(n)):\n",
    "            yield train_idx, test_idx\n",
    "\n",
    "\n",
    "    @property\n",
    "    def data(self):\n",
    "        X, y = [], []\n",
    "        for g in self.graphs:\n",
    "            X.append(g.text)\n",
    "            y.append(g.label)\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Dataset({self.name}, graphs={len(self.graphs)})'\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return self.graphs[key]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(self.graphs)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "    \n",
    "    def save(self):\n",
    "        print(f'Saving {self.name} to pickle')\n",
    "        with open(os.path.join(self.save_dir, f'{self.name}.pkl'), 'wb') as f:\n",
    "            pickle.dump(self.graphs, f)\n",
    "        print(f'Saved {self.name} to pickle')\n",
    "\n",
    "\n",
    "    def load(self):\n",
    "        print(f'Loading {self.name} from pickle')\n",
    "        with open(os.path.join(self.save_dir, f'{self.name}.pkl'), 'rb') as f:\n",
    "            self.graphs = pickle.load(f)\n",
    "        \n",
    "        print(f'Loaded {self.name} from pickle')\n",
    "    \n",
    "\n",
    "reload = False\n",
    "ecore = Dataset('ecore_555', reload=reload)\n",
    "modelset = Dataset('modelset', reload=reload, remove_duplicates=True)\n",
    "# mar = Dataset('mar-ecore-github', reload=reload)\n",
    "\n",
    "\n",
    "datasets = {\n",
    "    'ecore': ecore,\n",
    "    'modelset': modelset,\n",
    "    # 'mar': mar\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PrimitiveTypes\\nBoolean\\nInteger\\nString\\nBIBTEX\\nLocatedElement\\nlocation\\ncommentsBefore\\ncommentsAfter\\nBibtex\\nentries\\nEntry\\nkey\\nfields\\nArticle\\nBook\\nInbook\\nBooklet\\nInproceedings\\nProceedings\\nIncollection\\nTechreport\\nPhdThesis\\nMastersThesis\\nManual\\nMisc\\nField\\nvalue\\nAuthors\\nAuthorUrls\\nTitle\\nJournal\\nBookTitle\\nInstitution\\nOrganization\\nType\\nDay\\nNumber\\nChapter\\nVolume\\nSeries\\nPages\\nPublisher\\nHowpublished\\nSchool\\nEditor\\nEdition\\nAddress\\nYear\\nMonth\\nNote\\nText\\nAbstractField\\nIsbn\\nIssn\\nUrl\\nDoi'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecore[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PrimitiveTypes\\nBoolean\\nInteger\\nString\\nBIBTEX\\nLocatedElement\\nBibtex\\nEntry\\nArticle\\nBook\\nInbook\\nBooklet\\nInproceedings\\nProceedings\\nIncollection\\nTechreport\\nPhdThesis\\nMastersThesis\\nManual\\nMisc\\nField\\nAuthors\\nAuthorUrls\\nTitle\\nJournal\\nBookTitle\\nInstitution\\nOrganization\\nType\\nDay\\nNumber\\nChapter\\nVolume\\nSeries\\nPages\\nPublisher\\nHowpublished\\nSchool\\nEditor\\nEdition\\nAddress\\nYear\\nMonth\\nNote\\nText\\nAbstractField\\nIsbn\\nIssn\\nUrl\\nDoi\\nlocation\\ncommentsBefore\\ncommentsAfter\\nentries\\nkey\\nfields\\nvalue'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\\n\".join([d['name'] for n, d in ecore[0].nodes(data=True) if 'name' in d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ecore 548\n",
      "modelset 3337\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for name, dataset in datasets.items():\n",
    "    print(name, len(dataset))\n",
    "    df = pd.DataFrame([g.json_obj for g in dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29007"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max(modelset, key=lambda x: len(x.text)).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fasttext classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "for name, dataset in datasets.items():\n",
    "    if name not in ['ecore', 'modelset']:\n",
    "        continue\n",
    "    print(\"Dataset: \", name)\n",
    "    i = 0\n",
    "    accuracies, bal_accuracies = [], []\n",
    "    for X_train, X_test, y_train, y_test in dataset.k_fold_split():\n",
    "        print(\"Fold number: \", i+1)\n",
    "        f_train = f'datasets/fasttext_train_{name}_{i}.txt'\n",
    "        f_test = f'datasets/fasttext_test_{name}_{i}.txt'\n",
    "        if not os.path.exists(f_train):\n",
    "            with open(f_train, 'w') as f:\n",
    "                for x, y in zip(X_train, y_train):\n",
    "                    x = \" \".join(doc_tokenizer(x))\n",
    "                    f.write(f\"__label__{y} {x}\\n\")\n",
    "        \n",
    "        if not os.path.exists(f_test):\n",
    "            with open(f_test, 'w') as f:\n",
    "                for x, y in zip(X_test, y_test):\n",
    "                    x = \" \".join(doc_tokenizer(x))\n",
    "                    f.write(f\"__label__{y} {x}\\n\")\n",
    "        \n",
    "        if os.path.exists(f'models/{name}_{i}.bin'):\n",
    "            model = fasttext.load_model(f'models/{name}_{i}.bin')\n",
    "        else:\n",
    "            model = fasttext.train_supervised(\n",
    "                input=f_train, \n",
    "                epoch=100, \n",
    "                lr=0.2, \n",
    "                wordNgrams=2, \n",
    "            )\n",
    "            model.save_model(f'models/{name}_{i}.bin')        \n",
    "        y_pred = model.predict([i.strip() for i in open(f_test).readlines()])[0]\n",
    "        y_true = [i.split()[0].split('__label__')[1] for i in open(f_test).readlines()]\n",
    "        y_pred = [i[0].split('__label__')[1] for i in y_pred]\n",
    "\n",
    "\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        bal_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "        print(f\"Accuracy: {accuracy}, Balanced Accuracy: {bal_accuracy}\")\n",
    "        accuracies.append(accuracy)\n",
    "        bal_accuracies.append(bal_accuracy)\n",
    "\n",
    "        i += 1            \n",
    "    print(f\"Average Accuracy: {np.mean(accuracies)}, Average Balanced Accuracy: {np.mean(bal_accuracies)}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fasttext word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_udata = list(set([g.text for dataset in datasets.values() for g in dataset]))\n",
    "X_udata = [f\"{SEP}\".join(doc_tokenizer(x)) for x in X_udata]\n",
    "f_udata = 'datasets/fasttext_udata.txt'\n",
    "with open(f'{f_udata}', 'w') as f:\n",
    "    for x in X_udata:\n",
    "        f.write(f\"{x}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  8120\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:    7026 lr: -0.000001 avg.loss:  1.111646 ETA:   0h 0m 0s 60.3% words/sec/thread:    7034 lr:  0.039708 avg.loss:  1.177690 ETA:   0h 3m12s100.0% words/sec/thread:    7026 lr:  0.000000 avg.loss:  1.111496 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.train_unsupervised(\n",
    "    input=f_udata, \n",
    "    epoch=500, \n",
    "    lr=0.1,\n",
    "    minn=2,\n",
    "    maxn=5,\n",
    "    dim=128\n",
    ")\n",
    "model.save_model(\"models/uml_fasttext.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_encoder = TFIDFEncoder()\n",
    "bert_encoder = BertTokenizerEncoder('bert-base-uncased')\n",
    "bert_tfidf_encoder = BertTFIDF('bert-base-uncased')\n",
    "fasttext_encoder = FasttextEncoder('models/uml_fasttext.bin')\n",
    "class_label_encoder = ClassLabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "def train_svm(dataset: Dataset, encoder: Union[TFIDFEncoder, BertTFIDF, FasttextEncoder]):\n",
    "    accuracies, bal_accuracies = [], []\n",
    "    for train_idx, test_idx in dataset.k_fold_split():\n",
    "        X = encoder.encode(dataset.data[0])\n",
    "        y = class_label_encoder.encode(dataset.data[1])\n",
    "\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        svm_classifier = svm.SVC(kernel='linear')  # You can change the kernel as needed\n",
    "        svm_classifier.fit(X_train, y_train)\n",
    "        # Predict on the test set\n",
    "        y_pred = svm_classifier.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        # print(f'SVM Classifier Accuracy: {accuracy}')\n",
    "        bal_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "        # print(f'SVM Classifier Balanced Accuracy: {bal_accuracy}')\n",
    "\n",
    "        accuracies.append(accuracy)\n",
    "        bal_accuracies.append(bal_accuracy)\n",
    "    \n",
    "    print(f'Mean Accuracy: {np.mean(accuracies)}')\n",
    "    print(f'Mean Balanced Accuracy: {np.mean(bal_accuracies)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_svm(modelset, tf_idf_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.5969380140304565, 'petrinetv3'),\n",
       " (0.5963557362556458, 'petrinetv1'),\n",
       " (0.5946762561798096, 'petrinetv2'),\n",
       " (0.5399251580238342, 'petri'),\n",
       " (0.5047121047973633, 'tokens')]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_nearest_neighbors('petrinet', k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def split_into_chunks(text, max_length=512):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = [tokens[i:i + max_length] for i in range(0, len(tokens), max_length)]\n",
    "    return [' '.join(chunk) for chunk in chunks]\n",
    "\n",
    "# # Example usage\n",
    "long_text = max(modelset, key=lambda x: len(x.text)).text\n",
    "chunks = split_into_chunks(long_text)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    acc = accuracy_score(p.label_ids, preds)\n",
    "    balanced_acc = balanced_accuracy_score(p.label_ids, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'balanced_accuracy': balanced_acc,\n",
    "    }\n",
    "\n",
    "# Create your dataset\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        inputs = self.tokenizer(\n",
    "            text, \n",
    "            return_tensors='pt', \n",
    "            truncation=True, \n",
    "            padding='max_length', \n",
    "            max_length=self.max_length\n",
    "        )\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        inputs['labels'] = torch.tensor(label, dtype=torch.long)\n",
    "        return inputs\n",
    "\n",
    "\n",
    "def train_hf(model_name, model_ds: Dataset):\n",
    "    i = 0\n",
    "    print(f'Device used: {device}')\n",
    "\n",
    "    for train_idx, test_idx in model_ds.k_fold_split():\n",
    "        print(f'Fold number: {i+1}')\n",
    "        X, y = model_ds.data\n",
    "        y = LabelEncoder().fit_transform(y)\n",
    "        X_train, X_test = [X[i] for i in train_idx], [X[i] for i in test_idx]\n",
    "        y_train, y_test = [y[i] for i in train_idx], [y[i] for i in test_idx]\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(set(y)))\n",
    "        model.to(device)\n",
    "\n",
    "        train_ds = CustomDataset(X_train, y_train, tokenizer, max_length=4096)\n",
    "        test_ds = CustomDataset(X_test, y_test, tokenizer, max_length=4096)\n",
    "\n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir='./results',\n",
    "            num_train_epochs=10,\n",
    "            per_device_train_batch_size=2,\n",
    "            per_device_eval_batch_size=2,\n",
    "            warmup_steps=500,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir='./logs',\n",
    "            logging_steps=10,\n",
    "        )\n",
    "\n",
    "        # Trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_ds,\n",
    "            eval_dataset=test_ds  # Replace with actual evaluation dataset\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "        trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used: cuda\n",
      "Fold number: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 85, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1916, in forward\n    outputs = self.longformer(\n  File \"/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1729, in forward\n    encoder_outputs = self.encoder(\n  File \"/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1309, in forward\n    layer_outputs = layer_module(\n  File \"/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1237, in forward\n    self_attn_outputs = self.attention(\n  File \"/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1173, in forward\n    self_outputs = self.self(\n  File \"/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py\", line 642, in forward\n    attn_output = self._compute_attn_output_with_global_indices(\n  File \"/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1009, in _compute_attn_output_with_global_indices\n    attn_output_without_global = self._sliding_chunks_matmul_attn_probs_value(\n  File \"/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py\", line 890, in _sliding_chunks_matmul_attn_probs_value\n    chunked_attn_probs = attn_probs.transpose(1, 2).reshape(\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 194.00 MiB. GPU 0 has a total capacty of 23.68 GiB of which 97.75 MiB is free. Process 3515915 has 2.84 GiB memory in use. Including non-PyTorch memory, this process has 20.74 GiB memory in use. Of the allocated memory 19.50 GiB is allocated by PyTorch, and 866.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_hf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mallenai/longformer-base-4096\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mecore\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 84\u001b[0m, in \u001b[0;36mtrain_hf\u001b[0;34m(model_name, model_ds)\u001b[0m\n\u001b[1;32m     76\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     77\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     78\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     79\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_ds,\n\u001b[1;32m     80\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtest_ds  \u001b[38;5;66;03m# Replace with actual evaluation dataset\u001b[39;00m\n\u001b[1;32m     81\u001b[0m )\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[0;32m~/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/trainer.py:1932\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1930\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1933\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1937\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/trainer.py:2268\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2267\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2268\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2271\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2272\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2273\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2274\u001b[0m ):\n\u001b[1;32m   2275\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2276\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/trainer.py:3307\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3306\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3307\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3309\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3311\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/trainer.py:3338\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3336\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3337\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3338\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3339\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3340\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:185\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    184\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 185\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:200\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:110\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    108\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m--> 110\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/torch/_utils.py:694\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 694\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 85, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1916, in forward\n    outputs = self.longformer(\n  File \"/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1729, in forward\n    encoder_outputs = self.encoder(\n  File \"/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1309, in forward\n    layer_outputs = layer_module(\n  File \"/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1237, in forward\n    self_attn_outputs = self.attention(\n  File \"/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1173, in forward\n    self_outputs = self.self(\n  File \"/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py\", line 642, in forward\n    attn_output = self._compute_attn_output_with_global_indices(\n  File \"/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1009, in _compute_attn_output_with_global_indices\n    attn_output_without_global = self._sliding_chunks_matmul_attn_probs_value(\n  File \"/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py\", line 890, in _sliding_chunks_matmul_attn_probs_value\n    chunked_attn_probs = attn_probs.transpose(1, 2).reshape(\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 194.00 MiB. GPU 0 has a total capacty of 23.68 GiB of which 97.75 MiB is free. Process 3515915 has 2.84 GiB memory in use. Including non-PyTorch memory, this process has 20.74 GiB memory in use. Of the allocated memory 19.50 GiB is allocated by PyTorch, and 866.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "train_hf('allenai/longformer-base-4096', ecore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
