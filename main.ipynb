{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "datasets_dir = 'datasets'\n",
    "ecore_json_path = os.path.join(datasets_dir, 'ecore_555/ecore_555.jsonl')\n",
    "mar_json_path = os.path.join(datasets_dir, 'mar-ecore-github/ecore-github.jsonl')\n",
    "modelsets_uml_json_path = os.path.join(datasets_dir, 'modelset/uml.jsonl')\n",
    "modelsets_ecore_json_path = os.path.join(datasets_dir, 'modelset/ecore.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import fasttext\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from re import finditer\n",
    "\n",
    "\n",
    "SEP = ' '\n",
    "def camel_case_split(identifier):\n",
    "    matches = finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
    "    return [m.group(0) for m in matches]\n",
    "\n",
    "\n",
    "def doc_tokenizer(doc):\n",
    "    words = doc.split()\n",
    "    # split _\n",
    "    words = [w2 for w1 in words for w2 in w1.split('_') if w2 != '']\n",
    "    # camelcase\n",
    "    words = [w2.lower() for w1 in words for w2 in camel_case_split(w1) if w2 != '']\n",
    "    return words\n",
    "\n",
    "\n",
    "class TFIDFEncoder:\n",
    "    def __init__(self, X=None):\n",
    "        self.encoder = TfidfVectorizer(\n",
    "            lowercase=False, tokenizer=doc_tokenizer, min_df=3\n",
    "        )\n",
    "\n",
    "        if X:\n",
    "            self.encode(X)\n",
    "\n",
    "    def encode(self, X):\n",
    "        # print('Fitting TFIDF')\n",
    "        X_t = self.encoder.fit_transform(X)\n",
    "        X_sp = csr_matrix(np.vstack([x.toarray() for x in X_t]))\n",
    "        # print('TFIDF Encoded')\n",
    "        return X_sp\n",
    "\n",
    "\n",
    "class BertTokenizerEncoder:\n",
    "    def __init__(self, name, X=None):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "\n",
    "        if X:\n",
    "            self.encode(X)\n",
    "        \n",
    "\n",
    "    def encode(self, X, batch_encode=False, percentile=100):\n",
    "        # print('Tokenizing Bert')\n",
    "        tokens = self.tokenizer(X)\n",
    "\n",
    "        if batch_encode:\n",
    "            lengths = [len(i) for i in tokens['input_ids']]\n",
    "            size = int(np.percentile(lengths, percentile)) if percentile < 100 else max(lengths)\n",
    "            if size > 512:\n",
    "                print(f'WARNING: Max size is {size}. Truncating to 512')\n",
    "            size = max(size, 512)\n",
    "            \n",
    "            tokenized_data = self.tokenizer(\n",
    "                X, \n",
    "                padding=True, \n",
    "                truncation=True, \n",
    "                max_length=size\n",
    "            )\n",
    "        else:\n",
    "            tokenized_data = self.tokenizer(X)\n",
    "        # print('Bert Tokenized')\n",
    "\n",
    "        return tokenized_data\n",
    "\n",
    "\n",
    "class BertTFIDF:\n",
    "    def __init__(self, name, X=None):\n",
    "        self.bert = BertTokenizerEncoder(name)\n",
    "        self.tfidf = TFIDFEncoder()\n",
    "\n",
    "        if X:\n",
    "            self.encode(X)\n",
    "\n",
    "    def encode(self, X):\n",
    "        X_b = [f\"{SEP}\".join([str(j) for j in i]) for i in self.bert.encode(X)['input_ids']]\n",
    "        X_t = self.tfidf.encode(X_b)\n",
    "        return X_t\n",
    "\n",
    "\n",
    "class FasttextEncoder:\n",
    "    def __init__(self, model_name, X=None):\n",
    "        self.model = fasttext.load_model(model_name)\n",
    "        if X:\n",
    "            self.encode(X)\n",
    "\n",
    "    def encode(self, X):\n",
    "        def get_sentence_embedding(sentence):\n",
    "            return self.model.get_sentence_vector(sentence)\n",
    "        \n",
    "        # print('Encoding Fasttext')\n",
    "        X_t = [\" \".join(doc_tokenizer(i)) for i in X]\n",
    "        X_t = np.array([get_sentence_embedding(i) for i in X_t])\n",
    "        # print('Fasttext Encoded')\n",
    "        return X_t\n",
    "\n",
    "\n",
    "class ClassLabelEncoder(LabelEncoder):\n",
    "    def __init__(self, y=None) -> None:\n",
    "        super().__init__()\n",
    "        if y:\n",
    "            self.fit(y)\n",
    "    \n",
    "    def encode(self, y):\n",
    "        return self.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be26dbaae7794b1bbf0206ffc04dbb0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading Ecore_555:   0%|          | 0/548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ecore_555 to pickle\n",
      "Saved ecore_555 to pickle\n",
      "Graphs: 548\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2977fbd22db2466a9456224c25dde27d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading Modelset:   0%|          | 0/4127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving modelset to pickle\n",
      "Saved modelset to pickle\n",
      "Graphs: 2043\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c459d8159346868b43a0dd35d0a989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading Mar-Ecore-Github:   0%|          | 0/18110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving mar-ecore-github to pickle\n",
      "Saved mar-ecore-github to pickle\n",
      "Graphs: 18110\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import networkx as nx\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from abc import abstractmethod\n",
    "\n",
    "SEP = ' '\n",
    "\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "EGenericType = 'EGenericType'\n",
    "EPackage = 'EPackage'\n",
    "EClass = 'EClass'\n",
    "EAttribute = 'EAttribute'\n",
    "EReference = 'EReference'\n",
    "EEnum = 'EEnum'\n",
    "EEnumLiteral = 'EEnumLiteral'\n",
    "EOperation = 'EOperation'\n",
    "EParameter = 'EParameter'\n",
    "EDataType = 'EDataType'\n",
    "GenericNodes = [EGenericType, EPackage]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LangGraph(nx.DiGraph):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @abstractmethod\n",
    "    def create_graph(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_graph_node_text(self, node):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def find_node_str_upto_distance(self, node, distance=1):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_node_texts(self, distance=1):\n",
    "        pass\n",
    "\n",
    "    def find_nodes_within_distance(self, n, distance=1):\n",
    "        visited = {n: 0}\n",
    "        queue = [(n, 0)]\n",
    "        \n",
    "        while queue:\n",
    "            node, d = queue.pop(0)\n",
    "            if d == distance:\n",
    "                continue\n",
    "            for neighbor in self.neighbors(node):\n",
    "                if neighbor not in visited:\n",
    "                    visited[neighbor] = d+1\n",
    "                    queue.append((neighbor, d+1))\n",
    "        \n",
    "        visited = sorted(visited.items(), key=lambda x: x[1])\n",
    "        return visited\n",
    "\n",
    "\n",
    "class EcoreNxG(LangGraph):\n",
    "    def __init__(\n",
    "            self, \n",
    "            json_obj: dict, \n",
    "            use_type=True,\n",
    "            remove_generic_nodes=True,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.use_type = use_type\n",
    "        self.remove_generic_nodes = remove_generic_nodes\n",
    "        self.json_obj = json_obj\n",
    "        self.graph_id = json_obj.get('ids')\n",
    "        self.graph_type = json_obj.get('model_type')\n",
    "        self.label = json_obj.get('labels')\n",
    "        self.is_duplicated = json_obj.get('is_duplicated')\n",
    "        self.directed = json.loads(json_obj.get('graph')).get('directed')\n",
    "        self.create_graph(json_obj)\n",
    "        self.text = json_obj.get('txt')\n",
    "\n",
    "\n",
    "    def create_graph(self, json_obj):\n",
    "        generic_nodes = list()\n",
    "        graph = json.loads(json_obj['graph'])\n",
    "        nodes = graph['nodes']\n",
    "        edges = graph['links']\n",
    "        for node in nodes:\n",
    "            self.add_node(node['id'], **node)\n",
    "            if node['eClass'] in GenericNodes:\n",
    "                generic_nodes.append(node['id'])\n",
    "                \n",
    "        for edge in edges:\n",
    "            self.add_edge(edge['source'], edge['target'], **edge)\n",
    "        \n",
    "        if self.remove_generic_nodes:\n",
    "            self.remove_nodes_from(generic_nodes)\n",
    "    \n",
    "    def get_graph_node_text(self, node):\n",
    "        data = self.nodes[node]\n",
    "        node_class = data.get('eClass')\n",
    "        node_name = data.get('name', '')\n",
    "\n",
    "        if self.use_type:\n",
    "            return f'{node_class}({node_name})'\n",
    "\n",
    "        return node_name\n",
    "\n",
    "\n",
    "    def find_node_str_upto_distance(self, node, distance=1):\n",
    "        nodes_with_distance = self.find_nodes_within_distance(\n",
    "            node, \n",
    "            distance=distance\n",
    "        )\n",
    "        d2n = {d: set() for _, d in nodes_with_distance}\n",
    "        for n, d in nodes_with_distance:\n",
    "            node_text = self.get_graph_node_text(n)\n",
    "            if node_text:\n",
    "                d2n[d].add(node_text)\n",
    "        \n",
    "        d2n = sorted(d2n.items(), key=lambda x: x[0])\n",
    "        node_buckets = [f\"{SEP}\".join(nbs) for _, nbs in d2n]\n",
    "        path_str = \" | \".join(node_buckets)\n",
    "        \n",
    "        return path_str\n",
    "\n",
    "\n",
    "    def get_node_texts(self, distance=1):\n",
    "        node_texts = []\n",
    "        for node in self.nodes:\n",
    "            node_texts.append(\n",
    "                self.find_node_str_upto_distance(node, distance=distance)\n",
    "            )\n",
    "        \n",
    "        return node_texts\n",
    "    \n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'{self.json_obj}\\nGraph({self.graph_id}, nodes={self.number_of_nodes()}, edges={self.number_of_edges()})'\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(\n",
    "            self, \n",
    "            dataset_name: str, \n",
    "            dataset_dir = datasets_dir,\n",
    "            save_dir = 'datasets/pickles',\n",
    "            reload=False,\n",
    "            remove_duplicates=False,\n",
    "            use_type=False,\n",
    "            remove_generic_nodes=False,\n",
    "            extension='.jsonl'\n",
    "        ):\n",
    "        self.name = dataset_name\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.save_dir = save_dir\n",
    "        self.extension = extension\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        dataset_exists = os.path.exists(os.path.join(save_dir, f'{dataset_name}.pkl'))\n",
    "        if reload or not dataset_exists:\n",
    "            self.graphs: List[EcoreNxG] = []\n",
    "            data_path = os.path.join(dataset_dir, dataset_name)\n",
    "            for file in os.listdir(data_path):\n",
    "                if file.endswith(self.extension) and file.startswith('ecore'):\n",
    "                    json_objects = json.load(open(os.path.join(data_path, file)))\n",
    "                    self.graphs += [\n",
    "                        EcoreNxG(\n",
    "                            g, \n",
    "                            use_type=use_type, \n",
    "                            remove_generic_nodes=remove_generic_nodes\n",
    "                        ) for g in tqdm(json_objects, desc=f'Loading {dataset_name.title()}')\n",
    "                    ]\n",
    "            self.save()\n",
    "        \n",
    "        else:\n",
    "            self.load()\n",
    "        \n",
    "        if remove_duplicates:\n",
    "            self.remove_duplicates()\n",
    "\n",
    "        print(f'Graphs: {len(self.graphs)}')\n",
    "\n",
    "\n",
    "    def remove_duplicates(self):\n",
    "        self.graphs = self.dedup()\n",
    "\n",
    "    def dedup(self) -> List[EcoreNxG]:\n",
    "        return [g for g in self.graphs if not g.is_duplicated]\n",
    "    \n",
    "    \n",
    "    def get_train_test_split(self, train_size=0.8):\n",
    "        n = len(self.graphs)\n",
    "        train_size = int(n * train_size)\n",
    "        idx = list(range(n))\n",
    "        shuffle(idx)\n",
    "        train_idx = idx[:train_size]\n",
    "        test_idx = idx[train_size:]\n",
    "        return train_idx, test_idx\n",
    "    \n",
    "\n",
    "    def k_fold_split(\n",
    "            self,  \n",
    "            k=10\n",
    "        ):\n",
    "        kfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "        n = len(self.graphs)\n",
    "        for train_idx, test_idx in kfold.split(np.zeros(n), np.zeros(n)):\n",
    "            yield train_idx, test_idx\n",
    "\n",
    "\n",
    "    @property\n",
    "    def data(self):\n",
    "        X, y = [], []\n",
    "        for g in self.graphs:\n",
    "            X.append(g.text)\n",
    "            y.append(g.label)\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Dataset({self.name}, graphs={len(self.graphs)})'\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return self.graphs[key]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(self.graphs)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "    \n",
    "    def save(self):\n",
    "        print(f'Saving {self.name} to pickle')\n",
    "        with open(os.path.join(self.save_dir, f'{self.name}.pkl'), 'wb') as f:\n",
    "            pickle.dump(self.graphs, f)\n",
    "        print(f'Saved {self.name} to pickle')\n",
    "\n",
    "\n",
    "    def load(self):\n",
    "        print(f'Loading {self.name} from pickle')\n",
    "        with open(os.path.join(self.save_dir, f'{self.name}.pkl'), 'rb') as f:\n",
    "            self.graphs = pickle.load(f)\n",
    "        \n",
    "        print(f'Loaded {self.name} from pickle')\n",
    "    \n",
    "\n",
    "reload = False\n",
    "ecore = Dataset('ecore_555', reload=reload)\n",
    "modelset = Dataset('modelset', reload=reload, remove_duplicates=True)\n",
    "mar = Dataset('mar-ecore-github', reload=reload)\n",
    "\n",
    "\n",
    "datasets = {\n",
    "    'ecore': ecore,\n",
    "    'modelset': modelset,\n",
    "    'mar': mar\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Article | Entry BIBTEX | key Publisher Journal Chapter Url Type MastersThesis Title Inproceedings Volume Address Authors Proceedings Issn Day LocatedElement Institution Number Manual Edition Text Inbook Techreport Month AbstractField Field PhdThesis Howpublished Year Booklet Isbn Doi Organization Series AuthorUrls School Book Editor Bibtex Incollection fields Misc Pages Note BookTitle'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecore[0].find_node_str_upto_distance(8, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fasttext classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "for name, dataset in datasets.items():\n",
    "    if name not in ['ecore', 'modelset']:\n",
    "        continue\n",
    "    print(\"Dataset: \", name)\n",
    "    i = 0\n",
    "    accuracies, bal_accuracies = [], []\n",
    "    for X_train, X_test, y_train, y_test in dataset.k_fold_split():\n",
    "        print(\"Fold number: \", i+1)\n",
    "        f_train = f'datasets/fasttext_train_{name}_{i}.txt'\n",
    "        f_test = f'datasets/fasttext_test_{name}_{i}.txt'\n",
    "        if not os.path.exists(f_train):\n",
    "            with open(f_train, 'w') as f:\n",
    "                for x, y in zip(X_train, y_train):\n",
    "                    x = \" \".join(doc_tokenizer(x))\n",
    "                    f.write(f\"__label__{y} {x}\\n\")\n",
    "        \n",
    "        if not os.path.exists(f_test):\n",
    "            with open(f_test, 'w') as f:\n",
    "                for x, y in zip(X_test, y_test):\n",
    "                    x = \" \".join(doc_tokenizer(x))\n",
    "                    f.write(f\"__label__{y} {x}\\n\")\n",
    "        \n",
    "        if os.path.exists(f'models/{name}_{i}.bin'):\n",
    "            model = fasttext.load_model(f'models/{name}_{i}.bin')\n",
    "        else:\n",
    "            model = fasttext.train_supervised(\n",
    "                input=f_train, \n",
    "                epoch=100, \n",
    "                lr=0.2, \n",
    "                wordNgrams=2, \n",
    "            )\n",
    "            model.save_model(f'models/{name}_{i}.bin')        \n",
    "        y_pred = model.predict([i.strip() for i in open(f_test).readlines()])[0]\n",
    "        y_true = [i.split()[0].split('__label__')[1] for i in open(f_test).readlines()]\n",
    "        y_pred = [i[0].split('__label__')[1] for i in y_pred]\n",
    "\n",
    "\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        bal_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "        print(f\"Accuracy: {accuracy}, Balanced Accuracy: {bal_accuracy}\")\n",
    "        accuracies.append(accuracy)\n",
    "        bal_accuracies.append(bal_accuracy)\n",
    "\n",
    "        i += 1            \n",
    "    print(f\"Average Accuracy: {np.mean(accuracies)}, Average Balanced Accuracy: {np.mean(bal_accuracies)}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fasttext word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_udata = list(set([g.text for dataset in datasets.values() for g in dataset]))\n",
    "X_udata = [f\"{SEP}\".join(doc_tokenizer(x)) for x in X_udata]\n",
    "f_udata = 'datasets/fasttext_udata.txt'\n",
    "with open(f'{f_udata}', 'w') as f:\n",
    "    for x in X_udata:\n",
    "        f.write(f\"{x}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  8120\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:    7026 lr: -0.000001 avg.loss:  1.111646 ETA:   0h 0m 0s 60.3% words/sec/thread:    7034 lr:  0.039708 avg.loss:  1.177690 ETA:   0h 3m12s100.0% words/sec/thread:    7026 lr:  0.000000 avg.loss:  1.111496 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.train_unsupervised(\n",
    "    input=f_udata, \n",
    "    epoch=500, \n",
    "    lr=0.1,\n",
    "    minn=2,\n",
    "    maxn=5,\n",
    "    dim=128\n",
    ")\n",
    "model.save_model(\"models/uml_fasttext.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_encoder = TFIDFEncoder()\n",
    "bert_encoder = BertTokenizerEncoder('bert-base-uncased')\n",
    "bert_tfidf_encoder = BertTFIDF('bert-base-uncased')\n",
    "fasttext_encoder = FasttextEncoder('models/uml_fasttext.bin')\n",
    "class_label_encoder = ClassLabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    balanced_accuracy_score\n",
    ")\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "def train_svm(dataset: Dataset, encoder: Union[TFIDFEncoder, BertTFIDF, FasttextEncoder]):\n",
    "    accuracies, bal_accuracies = [], []\n",
    "    for train_idx, test_idx in dataset.k_fold_split():\n",
    "        X = encoder.encode(dataset.data[0])\n",
    "        y = class_label_encoder.encode(dataset.data[1])\n",
    "\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        svm_classifier = svm.SVC(kernel='linear')  # You can change the kernel as needed\n",
    "        svm_classifier.fit(X_train, y_train)\n",
    "        # Predict on the test set\n",
    "        y_pred = svm_classifier.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        # print(f'SVM Classifier Accuracy: {accuracy}')\n",
    "        bal_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "        # print(f'SVM Classifier Balanced Accuracy: {bal_accuracy}')\n",
    "\n",
    "        accuracies.append(accuracy)\n",
    "        bal_accuracies.append(bal_accuracy)\n",
    "    \n",
    "    print(f'Mean Accuracy: {np.mean(accuracies)}')\n",
    "    print(f'Mean Balanced Accuracy: {np.mean(bal_accuracies)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_svm(modelset, tf_idf_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.5969380140304565, 'petrinetv3'),\n",
       " (0.5963557362556458, 'petrinetv1'),\n",
       " (0.5946762561798096, 'petrinetv2'),\n",
       " (0.5399251580238342, 'petri'),\n",
       " (0.5047121047973633, 'tokens')]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_nearest_neighbors('petrinet', k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def split_into_chunks(text, max_length=512):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = [tokens[i:i + max_length] for i in range(0, len(tokens), max_length)]\n",
    "    return [' '.join(chunk) for chunk in chunks]\n",
    "\n",
    "# # Example usage\n",
    "long_text = max(modelset, key=lambda x: len(x.text)).text\n",
    "chunks = split_into_chunks(long_text)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    Trainer, \n",
    "    TrainingArguments\n",
    ")\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, \n",
    "    AutoTokenizer\n",
    ")\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from data_loading.dataset import Dataset\n",
    "from settings import device, seed\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from trainers.metrics import compute_metrics\n",
    "\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "max_length_map = {\n",
    "    'bert-base-uncased': 512,\n",
    "    'allenai/longformer-base-4096': 4096\n",
    "}\n",
    "\n",
    "\n",
    "# Create your dataset\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.inputs = tokenizer(\n",
    "            texts, \n",
    "            return_tensors='pt', \n",
    "            truncation=True, \n",
    "            padding='max_length', \n",
    "            max_length=max_length\n",
    "        )\n",
    "        self.inputs['labels'] = torch.tensor(labels, dtype=torch.long)\n",
    " \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs['input_ids'])\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = {key: val[index] for key, val in self.inputs.items()}\n",
    "        return item\n",
    "\n",
    "\n",
    "def train_hf(model_name, model_ds: Dataset, epochs):\n",
    "    max_len = max_length_map[model_name]\n",
    "    i = 0\n",
    "    print(f'Device used: {device}')\n",
    "\n",
    "    for train_idx, test_idx in model_ds.k_fold_split():\n",
    "        print(f'Fold number: {i+1}')\n",
    "        X, y = model_ds.data\n",
    "        print(f'X: {len(X)}, y: {len(y)}')\n",
    "        y = LabelEncoder().fit_transform(y)\n",
    "        X_train, X_test = [X[i] for i in train_idx], [X[i] for i in test_idx]\n",
    "        y_train, y_test = [y[i] for i in train_idx], [y[i] for i in test_idx]\n",
    "\n",
    "        print(f'Train: {len(X_train)}, Test: {len(X_test)}')\n",
    "\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(set(y)))\n",
    "        model.to(device)\n",
    "\n",
    "        train_ds = CustomDataset(X_train, y_train, tokenizer, max_length=max_len)\n",
    "        test_ds = CustomDataset(X_test, y_test, tokenizer, max_length=max_len)\n",
    "\n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir='./results',\n",
    "            num_train_epochs=epochs,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            per_device_train_batch_size=2,\n",
    "            per_device_eval_batch_size=2,\n",
    "            warmup_steps=500,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir='./logs',\n",
    "            logging_steps=10,\n",
    "            load_best_model_at_end=True,\n",
    "            save_total_limit=1,\n",
    "            fp16=True,\n",
    "            seed=42\n",
    "        )\n",
    "\n",
    "        # Trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_ds,\n",
    "            eval_dataset=test_ds,\n",
    "            compute_metrics=compute_metrics            \n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "        results = trainer.evaluate()\n",
    "        print(results)\n",
    "\n",
    "        i += 1\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hf('bert-base-uncased', modelset, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold number: 1\n",
      "X: 2043, y: 2043\n",
      "Train: 1838, Test: 205\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from settings import device\n",
    "from data_loading.dataset import EncodingDataset\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "max_len = 512\n",
    "\n",
    "i = 0\n",
    "for train_idx, test_idx in modelset.k_fold_split():\n",
    "    print(f'Fold number: {i+1}')\n",
    "    X, y = modelset.data\n",
    "    print(f'X: {len(X)}, y: {len(y)}')\n",
    "    y = LabelEncoder().fit_transform(y)\n",
    "    X_train, X_test = [X[i] for i in train_idx], [X[i] for i in test_idx]\n",
    "    y_train, y_test = [y[i] for i in train_idx], [y[i] for i in test_idx]\n",
    "\n",
    "    print(f'Train: {len(X_train)}, Test: {len(X_test)}')\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('results/checkpoint-1380', num_labels=len(set(y)))\n",
    "    model.to(device)\n",
    "\n",
    "    train_ds = EncodingDataset(tokenizer, X_train, y_train, max_length=max_len)\n",
    "    test_ds = EncodingDataset(tokenizer, X_test, y_test, max_length=max_len)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([205, 512])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds[:]['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    #### Put vaues of custom data on device\n",
    "\n",
    "    test_ds = {k: v.to(device) for k, v in test_ds[:].items()}\n",
    "\n",
    "    outputs = model(**test_ds)\n",
    "    pred_classes = torch.argmax(outputs.logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 27, 10]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = pred_classes.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "from typing import List, Union\n",
    "from data_loading.dataset import EncodingDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from settings import device\n",
    "\n",
    "\n",
    "from abc import abstractmethod\n",
    "from typing import List, Union\n",
    "\n",
    "\n",
    "class Embedder:\n",
    "\n",
    "    def ___init__(self):\n",
    "        self.finetuned = False\n",
    "\n",
    "    @abstractmethod\n",
    "    def embed(self, text: Union[str, List[str]], aggregate='mean'):\n",
    "        pass\n",
    "\n",
    "\n",
    "class BertEmbedder(Embedder):\n",
    "    def __init__(self, model_name, ckpt=None):\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(ckpt if ckpt else model_name)\n",
    "        self.model.to(device)\n",
    "        self.finetuned = ckpt is not None\n",
    "    \n",
    "    def embed(self, text: Union[str, List[str]], aggregate='mean'):\n",
    "        dataset = EncodingDataset(self.tokenizer, texts=text)\n",
    "        loader = DataLoader(dataset, batch_size=128)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embeddings = []\n",
    "            for batch in loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                outputs = self.model(input_ids, attention_mask)\n",
    "                embeddings.append(outputs.last_hidden_state)\n",
    "                \n",
    "            \n",
    "            embeddings = torch.cat(embeddings, dim=0)\n",
    "            if aggregate == 'mean':\n",
    "                embeddings = embeddings.mean(dim=1)\n",
    "            elif aggregate == 'max':\n",
    "                embeddings = embeddings.max(dim=1)\n",
    "            elif aggregate == 'cls':\n",
    "                embeddings = embeddings[:, 0, :]\n",
    "            elif aggregate == 'pool':\n",
    "                embeddings = embeddings.mean(dim=1)\n",
    "            else:\n",
    "                raise ValueError(f'Unknown aggregation method: {aggregate}') \n",
    "        \n",
    "        return embeddings.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_embedder = BertEmbedder('bert-base-uncased', 'results/checkpoint-1380')\n",
    "bert_embedder = BertEmbedder('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 27)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = modelset[0].get_node_texts()\n",
    "len(texts), modelset[0].number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "class TorchGraph:\n",
    "    def __init__(\n",
    "            self, \n",
    "            graph: LangGraph, \n",
    "            embedder: Embedder,\n",
    "            save_dir: str,\n",
    "        ):\n",
    "        self.graph = graph\n",
    "        self.embedder = embedder\n",
    "        \n",
    "        self.save_dir = save_dir\n",
    "        self.process_graph()\n",
    "    \n",
    "\n",
    "    def process_graph(self):\n",
    "        if self.load():\n",
    "            return\n",
    "        texts = self.graph.get_node_texts()\n",
    "        self.embeddings = self.embedder.embed(texts)\n",
    "        self.edge_index = torch.tensor(\n",
    "            list(self.graph.edges), dtype=torch.long).t().contiguous()\n",
    "        self.save()\n",
    "    \n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return '.'.join(self.graph.graph_id.replace('/', '_').split('.')[:-1])\n",
    "\n",
    "\n",
    "    @property\n",
    "    def save_path(self):\n",
    "        path = os.path.join(self.save_dir, f'{self.name}')\n",
    "        if self.embedder.finetuned:\n",
    "            path = f'{path}_finetuned'\n",
    "        return path\n",
    "\n",
    "\n",
    "    def load(self):\n",
    "        if os.path.exists(self.save_path):\n",
    "            self.embeddings = torch.load(f\"{self.save_path}/embeddings.pt\")\n",
    "            self.edge_index = torch.load(f\"{self.save_path}/edge_index.pt\")\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "\n",
    "    def save(self):\n",
    "        os.makedirs(self.save_path, exist_ok=True)\n",
    "        torch.save(self.embeddings, f\"{self.save_path}/embeddings.pt\")\n",
    "        torch.save(self.edge_index, f\"{self.save_path}/edge_index.pt\")\n",
    "\n",
    "\n",
    "class GraphDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            models_dataset: Dataset,\n",
    "            embedder: Embedder,\n",
    "            save_dir='datasets/graph_data',\n",
    "        ):\n",
    "        self.save_dir = f'{save_dir}/{models_dataset.name}'\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        self.graphs = [\n",
    "            TorchGraph(g, embedder, save_dir=self.save_dir) \n",
    "            for g in tqdm(models_dataset, desc=f'Processing {models_dataset.name}')\n",
    "        ]\n",
    "\n",
    "        self._c = {label:j for j, label in enumerate({g.label for g in models_dataset})}\n",
    "        self.labels = torch.tensor([self._c[g.label] for g in models_dataset], dtype=torch.long)\n",
    "        self.num_classes = len(self._c)\n",
    "        self.num_features = self.graphs[0].embeddings.shape[-1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        return Data(\n",
    "            x=self.graphs[index].embeddings,\n",
    "            edge_index=self.graphs[index].edge_index,\n",
    "            y=self.labels[index]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c31826ab6914f489b6bc7a82ad64cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing modelset:   0%|          | 0/2043 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f03b9fefe3374ffc8a92d40763be098d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing modelset:   0%|          | 0/2043 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph_dataset_ft = GraphDataset(modelset, ft_embedder)\n",
    "graph_dataset = GraphDataset(modelset, bert_embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, test_idx = modelset.get_train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = [graph_dataset_ft[i] for i in train_idx]\n",
    "test_dataset = [graph_dataset_ft[i] for i in test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, global_mean_pool\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class GraphSAGEBlock(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, dropout):\n",
    "        super(GraphSAGEBlock, self).__init__()\n",
    "        self.conv = SAGEConv(in_dim, out_dim, aggr='sum')\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            input_dim, \n",
    "            hidden_dim, \n",
    "            output_dim, \n",
    "            num_layers, \n",
    "            dropout=0.1\n",
    "        ):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.gnn = nn.ModuleList([\n",
    "            GraphSAGEBlock(\n",
    "                input_dim if i == 0 else hidden_dim, \n",
    "                hidden_dim, \n",
    "                dropout\n",
    "            ) for i in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        for gnn_layer in self.gnn:\n",
    "            x = gnn_layer(x, edge_index)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "def train(loader):\n",
    "    avg_loss = 0\n",
    "    model.train()\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss += loss.item()\n",
    "    \n",
    "    return avg_loss / len(loader)\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += pred.eq(data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "\n",
    "train_idx, test_idx = modelset.get_train_test_split()\n",
    "\n",
    "batch_size = 32\n",
    "hidden_dim = 64\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Training the model\n",
    "model = GraphSAGE(\n",
    "    input_dim=graph_dataset_ft.num_features,\n",
    "    hidden_dim=hidden_dim, \n",
    "    output_dim=graph_dataset_ft.num_classes,\n",
    "    num_layers=3,\n",
    ").to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 201):\n",
    "    loss = train(train_dataloader)\n",
    "    test_acc = test(test_dataloader)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:03f} Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
