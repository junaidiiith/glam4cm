{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading eamodelset from pickle\n",
      "Loaded eamodelset with 558 graphs\n",
      "Loaded eamodelset with 558 graphs\n",
      "Graphs: 558\n"
     ]
    }
   ],
   "source": [
    "from utils import set_seed\n",
    "from data_loading.models_dataset import ArchiMateModelDataset\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "config_params = dict(\n",
    "    min_enr = -1,\n",
    "    min_edges = 10,\n",
    "    language = 'en',\n",
    ")\n",
    "dataset_name = 'eamodelset'\n",
    "\n",
    "dataset = ArchiMateModelDataset(dataset_name, **config_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading graph dataset\n",
      "Using tokenizer\n",
      "Loading tokenizer for bert-base-uncased\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdb8c6a50ff24dccabffa620aca200c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Embeddings graphs:   0%|          | 0/558 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ed813761bd47d583e66225342acb32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating graphs:   0%|          | 0/558 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AndJunction' 'ApplicationCollaboration' 'ApplicationComponent'\n",
      " 'ApplicationEvent' 'ApplicationFunction' 'ApplicationInteraction'\n",
      " 'ApplicationInterface' 'ApplicationProcess' 'ApplicationService'\n",
      " 'Artifact' 'Assessment' 'BusinessActor' 'BusinessCollaboration'\n",
      " 'BusinessEvent' 'BusinessFunction' 'BusinessInteraction'\n",
      " 'BusinessInterface' 'BusinessObject' 'BusinessProcess' 'BusinessRole'\n",
      " 'BusinessService' 'Capability' 'CommunicationNetwork' 'Constraint'\n",
      " 'Contract' 'CourseOfAction' 'DataObject' 'Deliverable' 'Device'\n",
      " 'DistributionNetwork' 'Driver' 'Equipment' 'Facility' 'Gap' 'Goal'\n",
      " 'Grouping' 'ImplementationEvent' 'Junction' 'Location' 'Material'\n",
      " 'Meaning' 'Node' 'OrJunction' 'Outcome' 'Path' 'Plateau' 'Principle'\n",
      " 'Product' 'Representation' 'Requirement' 'Resource' 'Stakeholder'\n",
      " 'SystemSoftware' 'TechnologyCollaboration' 'TechnologyEvent'\n",
      " 'TechnologyFunction' 'TechnologyInteraction' 'TechnologyInterface'\n",
      " 'TechnologyProcess' 'TechnologyService' 'Value' 'ValueStream'\n",
      " 'WorkPackage' None]\n",
      "Setting num_nodes_ type 63\n",
      "['application' 'business' 'implementation_migration' 'motivation' 'other'\n",
      " 'strategy' 'technology' None]\n",
      "Setting num_nodes_ layer 7\n",
      "Edge Classes:  ['Access' 'Aggregation' 'Assignment' 'Association' 'Composition' 'Flow'\n",
      " 'Influence' 'Realization' 'Serving' 'Specialization' 'Triggering']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d66bfe278ca4b5881af31bd91f4f134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating edge classes:   0%|          | 0/558 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train classes: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n",
      "Test classes: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n",
      "Number of classes in training set: 11\n",
      "Number of classes in test set: 11\n",
      "Train edge classes: {4: 12783, 8: 5362, 7: 7065, 3: 7522, 2: 5802, 1: 3550, 10: 4397, 9: 1205, 5: 4413, 6: 692, 0: 4011}\n",
      "Test edge classes: {4: 3109, 7: 1808, 3: 1827, 8: 1395, 5: 1049, 10: 1100, 1: 848, 0: 956, 2: 1359, 9: 301, 6: 171}\n",
      "Loaded graph dataset\n"
     ]
    }
   ],
   "source": [
    "from data_loading.graph_dataset import GraphEdgeDataset, GraphNodeDataset\n",
    "\n",
    "graph_data_params = dict(\n",
    "    # reload=True,\n",
    "    test_ratio=0.2,\n",
    "    add_negative_train_samples=True,\n",
    "    neg_sampling_ratio=1,\n",
    "    distance=0,\n",
    "    use_special_tokens=True,\n",
    "    random_embed_dim=1,\n",
    "    # use_edge_types=True,\n",
    "    use_node_types=True,\n",
    "    use_embeddings=True,\n",
    "    # embed_model_name='bert-base-cased',\n",
    "    ckpt='results/eamodelset/lp/10_att_0_nt_0/checkpoint-177600',\n",
    "    limit = -1,\n",
    ")\n",
    "\n",
    "print(\"Loading graph dataset\")\n",
    "graph_edge_dataset = GraphEdgeDataset(dataset, **graph_data_params)\n",
    "print(\"Loaded graph dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(NumpyData(train_edge_mask=[40], test_edge_mask=[10], train_pos_edge_label_index=[2, 40], train_pos_edge_label=[40], train_neg_edge_label_index=[2, 40], train_neg_edge_label=[40], test_pos_edge_label_index=[2, 10], test_pos_edge_label=[10], test_neg_edge_label_index=[2, 10], test_neg_edge_label=[10], overall_edge_index=[2, 50], edge_index=[2, 40], num_nodes=62, num_edges=50, x=[62, 832], edge_attr=[50, 768], node_type=[62], node_layer=[62], edge_type=[50]),\n",
       " GraphData(train_edge_mask=[40], test_edge_mask=[10], train_pos_edge_label_index=[2, 40], train_pos_edge_label=[40], train_neg_edge_label_index=[2, 40], train_neg_edge_label=[40], test_pos_edge_label_index=[2, 10], test_pos_edge_label=[10], test_neg_edge_label_index=[2, 10], test_neg_edge_label=[10], overall_edge_index=[2, 50], edge_index=[2, 40], num_nodes=62, num_edges=50, x=[62, 832], edge_attr=[50, 768], node_type=[62], node_layer=[62], edge_type=[50]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_edge_dataset.graphs[0].data, graph_edge_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_data = graph_edge_dataset[0].get_link_prediction_texts('type', 'lp')\n",
    "texts_data['test_pos_edges'][:20], texts_data['test_neg_edges'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets/graph_data/eamodelset/edge_data_dist_0_neg_nsr_1_nt_1_sp_1_use_emb_1_ckpt_results_eamodelset_lp_10_att_0_nt_0_checkpoint-177600_test_0.2/9256ec1b11002dd708b621c9cf26616b/data.pkl'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_edge_dataset.file_paths[dataset[0].hash]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dataset = graph_edge_dataset.get_torch_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([62, 1]), torch.Size([2, 40]), torch.Size([50, 1]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_dataset[0].x.shape, torch_dataset[0].edge_index.shape, torch_dataset[0].edge_attr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphData(train_edge_mask=[40], test_edge_mask=[10], train_pos_edge_label_index=[2, 40], train_pos_edge_label=[40], train_neg_edge_label_index=[2, 40], train_neg_edge_label=[40], test_pos_edge_label_index=[2, 10], test_pos_edge_label=[10], test_neg_edge_label_index=[2, 10], test_neg_edge_label=[10], overall_edge_index=[2, 50], edge_index=[2, 40], num_nodes=62, num_edges=50, node_type=[62], node_layer=[62], edge_type=[50], x=[62, 1], edge_attr=[50, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 20.1870\n",
      "Average Test Accuracy: 0.8861\n",
      "Epoch 2, Loss: 19.4412\n",
      "Average Test Accuracy: 0.8885\n",
      "Epoch 3, Loss: 19.1393\n",
      "Average Test Accuracy: 0.8865\n",
      "Epoch 4, Loss: 19.1695\n",
      "Average Test Accuracy: 0.8885\n",
      "Epoch 5, Loss: 19.2538\n",
      "Average Test Accuracy: 0.8878\n",
      "Epoch 6, Loss: 19.2151\n",
      "Average Test Accuracy: 0.8877\n",
      "Epoch 7, Loss: 19.0947\n",
      "Average Test Accuracy: 0.8873\n",
      "Epoch 8, Loss: 19.1740\n",
      "Average Test Accuracy: 0.8876\n",
      "Epoch 9, Loss: 19.2315\n",
      "Average Test Accuracy: 0.8875\n",
      "Epoch 10, Loss: 19.1580\n",
      "Average Test Accuracy: 0.8869\n",
      "Epoch 11, Loss: 19.2266\n",
      "Average Test Accuracy: 0.8882\n",
      "Epoch 12, Loss: 19.1967\n",
      "Average Test Accuracy: 0.8867\n",
      "Epoch 13, Loss: 19.2607\n",
      "Average Test Accuracy: 0.8866\n",
      "Epoch 14, Loss: 19.2285\n",
      "Average Test Accuracy: 0.8868\n",
      "Epoch 15, Loss: 19.2100\n",
      "Average Test Accuracy: 0.8881\n",
      "Epoch 16, Loss: 19.2172\n",
      "Average Test Accuracy: 0.8886\n",
      "Epoch 17, Loss: 19.2189\n",
      "Average Test Accuracy: 0.8867\n",
      "Epoch 18, Loss: 19.1475\n",
      "Average Test Accuracy: 0.8864\n",
      "Epoch 19, Loss: 19.2123\n",
      "Average Test Accuracy: 0.8890\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from models.gnn_layers import *\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import SAGEConv, GATv2Conv\n",
    "\n",
    "\n",
    "class GNNLinkPredictor(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super(GNNLinkPredictor, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.link_predictor = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2 * hidden_channels, hidden_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_channels, 2)  # Two output classes: True or False\n",
    "        )\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "    def decode(self, z, edge_index):\n",
    "        src, dst = edge_index\n",
    "        edge_features = torch.cat([z[src], z[dst]], dim=1)\n",
    "        return self.link_predictor(edge_features)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_pos):\n",
    "        z = self.encode(x, edge_index)\n",
    "        return self.decode(z, edge_pos)\n",
    "\n",
    "\n",
    "\n",
    "def train(model, data, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Use the training positive and negative edges\n",
    "    edge_pos = data.train_pos_edge_label_index\n",
    "    edge_neg = data.train_neg_edge_label_index\n",
    "\n",
    "    # Combine positive and negative edges\n",
    "    edge_index = torch.cat([edge_pos, edge_neg], dim=1)\n",
    "    labels = torch.cat([torch.ones(edge_pos.size(1)), torch.zeros(edge_neg.size(1))]).long().to(data.x.device)\n",
    "\n",
    "    # Forward pass\n",
    "    out = model(data.x, edge_pos, edge_index)\n",
    "    loss = F.cross_entropy(out, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data):\n",
    "    model.eval()\n",
    "\n",
    "    # Evaluate on test positive and negative edges\n",
    "    edge_pos = data.test_pos_edge_label_index\n",
    "    edge_neg = data.test_neg_edge_label_index\n",
    "    edge_index = torch.cat([edge_pos, edge_neg], dim=1)\n",
    "    labels = torch.cat([torch.ones(edge_pos.size(1)), torch.zeros(edge_neg.size(1))]).long().to(data.x.device)\n",
    "\n",
    "    # Forward pass\n",
    "    out = model(data.x, edge_pos, edge_index)\n",
    "    pred = out.argmax(dim=1)  # Get predicted class (0 or 1)\n",
    "    accuracy = (pred == labels).sum().item() / labels.size(0)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def run(dataloader, num_epochs=100):\n",
    "\t# Training loop\n",
    "\tfor epoch in range(1, num_epochs):\n",
    "\t\tloss = 0\n",
    "\t\tfor data in dataloader:  # Iterate over each graph in the dataset\n",
    "\t\t\tdata = data.to(device)\n",
    "\t\t\tloss += train(model, data, optimizer)\n",
    "\t\t\n",
    "\t\tprint(f'Epoch {epoch}, Loss: {loss:.4f}')\n",
    "\n",
    "\t\t# Test loop\n",
    "\t\ttotal_accuracy = 0\n",
    "\t\tfor data in dataloader:\n",
    "\t\t\tdata = data.to(device)\n",
    "\t\t\ttotal_accuracy += test(model, data)\n",
    "\t\taverage_accuracy = total_accuracy / len(dataloader)\n",
    "\t\tprint(f'Average Test Accuracy: {average_accuracy:.4f}')\n",
    "\n",
    "\n",
    "hidden_channels = 32\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GNNLinkPredictor(in_channels=torch_dataset[0].x.shape[-1], hidden_channels=hidden_channels).to(device)  # Adjust input size accordingly\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "dataloader = DataLoader(torch_dataset, batch_size=16, shuffle=True)\n",
    "run(dataloader, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'train_positive': 56802,\n",
       "             'train_negative': 56802,\n",
       "             'test_positive': 13923,\n",
       "             'test_negative': 13923})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "edges = defaultdict(int)\n",
    "\n",
    "for graph in graph_edge_dataset:\n",
    "\tpos_edge_idx = graph.data.edge_index\n",
    "\ttrain_pos_edge_index = pos_edge_idx\n",
    "\ttrain_neg_edge_index = graph.data.train_neg_edge_label_index\n",
    "\ttest_pos_edge_index = graph.data.test_pos_edge_label_index\n",
    "\ttest_neg_edge_index = graph.data.test_neg_edge_label_index\n",
    "\n",
    "\tassert set((a, b) for a, b in train_pos_edge_index.T.tolist()).issubset(set(graph.graph.numbered_graph.edges()))\n",
    "\tassert set((a, b) for a, b in test_pos_edge_index.T.tolist()).issubset(set(graph.graph.numbered_graph.edges()))\n",
    "\n",
    "\tassert len(set(graph.graph.numbered_graph.edges()).intersection(set((a, b) for a, b in test_neg_edge_index.T.tolist()))) == 0\n",
    "\tassert len(set(graph.graph.numbered_graph.edges()).intersection(set((a, b) for a, b in train_neg_edge_index.T.tolist()))) == 0\n",
    "\n",
    "\tassert len(set((a, b) for a, b in train_pos_edge_index.T.tolist()).intersection(set((a, b) for a, b in test_neg_edge_index.T.tolist()))) == 0\n",
    "\tassert len(set((a, b) for a, b in train_pos_edge_index.T.tolist()).intersection(set((a, b) for a, b in test_neg_edge_index.T.tolist()))) == 0\n",
    "\n",
    "\tedges['train_positive'] += train_pos_edge_index.size(1)\n",
    "\tedges['train_negative'] += train_neg_edge_index.size(1)\n",
    "\tedges['test_positive'] += test_pos_edge_index.size(1)\n",
    "\tedges['test_negative'] += test_neg_edge_index.size(1)\n",
    "\n",
    "edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data_params = dict(\n",
    "    # reload=True,\n",
    "    test_ratio=0.2,\n",
    "    distance=0,\n",
    "    use_edge_types=True,\n",
    "    use_special_tokens=True,\n",
    "    use_embeddings=True,\n",
    "    embed_model_name='bert-base-cased',\n",
    "    ckpt='results/eamodelset/edge_cls/type_10_att_0_nt_1/checkpoint-2200'\n",
    ")\n",
    "\n",
    "print(\"Loading graph dataset\")\n",
    "graph_node_dataset = GraphNodeDataset(dataset, **graph_data_params)\n",
    "print(\"Loaded graph dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_texts = graph_edge_dataset.get_link_prediction_texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_torch_data = graph_edge_dataset.get_torch_geometric_data(\n",
    "    # use_edge_types=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(dict(graph_node_dataset[0].graph.nodes(data='type')).values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphData(train_edge_mask=[34], test_edge_mask=[8], train_pos_edge_label_index=[2, 34], train_pos_edge_label=[34], train_neg_edge_label_index=[2, 34], train_neg_edge_label=[34], test_pos_edge_label_index=[2, 8], test_pos_edge_label=[8], test_neg_edge_label_index=[2, 8], test_neg_edge_label=[8], overall_edge_index=[2, 42], edge_index=[2, 34], x=[97, 768], edge_attr=[42, 768], num_nodes=97, num_edges=42, node_type=[97], node_layer=[97], edge_type=[42])\n"
     ]
    }
   ],
   "source": [
    "for torch_graph in graph_torch_data:\n",
    "    print(torch_graph)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "num_classes = getattr(graph_edge_dataset, 'num_edges_type')\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "for label in range(num_classes):\n",
    "    print(\"Label: \", label)\n",
    "    for torch_graph in graph_torch_data:\n",
    "        indices = (torch_graph.edge_type == label).nonzero(as_tuple=True)[0]\n",
    "        selected_embeddings = torch_graph.edge_attr[indices]\n",
    "        if len(selected_embeddings) == 0:\n",
    "            continue\n",
    "        comparison = torch.all(selected_embeddings == selected_embeddings[0], dim=1)\n",
    "        if torch.where(~comparison)[0].shape[0] > 0:\n",
    "            assert torch.where(~comparison)[0].shape[0] == 0, f\"Label {label} has different embeddings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 5, 0, 1, 2, 5])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "class GraphData(Data):\n",
    "    def __inc__(self, key, value, *args, **kwargs):\n",
    "        if 'index' in key or 'node_mask' in key:\n",
    "            return self.num_nodes\n",
    "        elif 'edge_mask' in key:\n",
    "            return self.num_edges\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def __cat_dim__(self, key, value, *args, **kwargs):\n",
    "        if 'index' in key:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "edge_index = torch.tensor([\n",
    "   [0, 1, 1, 2],\n",
    "   [1, 0, 2, 1],\n",
    "])\n",
    "edge_mask = torch.tensor([1, 0, 1, 0])\n",
    "label = torch.tensor([0, 1, 2, 5])\n",
    "foo = torch.randn(16)\n",
    "data = GraphData(num_nodes=3, edge_index=edge_index, foo=foo, edge_mask=edge_mask, label=label)\n",
    "data_list = [data, data]\n",
    "loader = DataLoader(data_list, batch_size=2)\n",
    "batch = next(iter(loader))\n",
    "print(batch.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_batch(batch, label):\n",
    "    print(\"Label: \", label)\n",
    "    train_mask = batch.train_edge_mask\n",
    "    test_mask = batch.test_edge_mask\n",
    "\n",
    "    labels = (batch.edge_type == label).nonzero(as_tuple=True)[0]\n",
    "    train_indices = labels[torch.isin(labels, train_mask)]\n",
    "    test_indices = labels[torch.isin(labels, test_mask)]\n",
    "\n",
    "    train_selected_embeddings = batch.edge_attr[train_indices]\n",
    "    test_selected_embeddings = batch.edge_attr[test_indices]\n",
    "    \n",
    "    if len(train_selected_embeddings) > 0:\n",
    "        train_cosine_similarity = torch.nn.functional.cosine_similarity(train_selected_embeddings, train_selected_embeddings[0].repeat(len(train_selected_embeddings), 1))\n",
    "        assert abs(int(sum(train_cosine_similarity)) - len(train_cosine_similarity)) <= 2, f\"Train Label {label} has different embeddings\"\n",
    "    \n",
    "    if len(test_selected_embeddings) > 0:\n",
    "        test_cosine_similarity = torch.nn.functional.cosine_similarity(test_selected_embeddings, test_selected_embeddings[0].repeat(len(test_selected_embeddings), 1))\n",
    "        assert abs(int(sum(test_cosine_similarity)) - len(test_cosine_similarity)) <= 2, f\"Test Label {label} has different embeddings\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def intersect_tensors(t1: torch.Tensor, t2: torch.Tensor) -> torch.Tensor:\n",
    "    # Use torch.isin to get a mask of elements in t1 that are also in t2\n",
    "    mask = torch.isin(t1, t2)\n",
    "    \n",
    "    # Use the mask to filter t1\n",
    "    t3 = t1[mask]\n",
    "    \n",
    "    return t3\n",
    "\n",
    "# Example usage:\n",
    "t1 = torch.tensor([1, 2, 3, 4, 5])\n",
    "t2 = torch.tensor([3, 4, 6, 7, 10, 12, 15, 14])\n",
    "\n",
    "t3 = intersect_tensors(t1, t2)\n",
    "print(t3)  # Output: tensor([3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  0\n",
      "Label:  1\n",
      "Label:  2\n",
      "Label:  3\n",
      "Label:  4\n",
      "Label:  5\n",
      "Label:  6\n",
      "Label:  7\n",
      "Label:  8\n",
      "Label:  9\n",
      "Label:  10\n"
     ]
    }
   ],
   "source": [
    "for label in range(num_classes):\n",
    "    check_batch(batch, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  0\n",
      "Label:  1\n",
      "Label:  2\n",
      "Label:  3\n",
      "Label:  4\n",
      "Label:  5\n",
      "Label:  6\n",
      "Label:  7\n",
      "Label:  8\n",
      "Label:  9\n",
      "Label:  10\n",
      "Label:  0\n",
      "Label:  1\n",
      "Label:  2\n",
      "Label:  3\n",
      "Label:  4\n",
      "Label:  5\n",
      "Label:  6\n",
      "Label:  7\n",
      "Label:  8\n",
      "Label:  9\n",
      "Label:  10\n",
      "Label:  0\n",
      "Label:  1\n",
      "Label:  2\n",
      "Label:  3\n",
      "Label:  4\n",
      "Label:  5\n",
      "Label:  6\n",
      "Label:  7\n",
      "Label:  8\n",
      "Label:  9\n",
      "Label:  10\n",
      "Label:  0\n",
      "Label:  1\n",
      "Label:  2\n",
      "Label:  3\n",
      "Label:  4\n",
      "Label:  5\n",
      "Label:  6\n",
      "Label:  7\n",
      "Label:  8\n",
      "Label:  9\n",
      "Label:  10\n",
      "Label:  0\n",
      "Label:  1\n",
      "Label:  2\n",
      "Label:  3\n",
      "Label:  4\n",
      "Label:  5\n",
      "Label:  6\n",
      "Label:  7\n",
      "Label:  8\n",
      "Label:  9\n",
      "Label:  10\n",
      "Label:  0\n",
      "Label:  1\n",
      "Label:  2\n",
      "Label:  3\n",
      "Label:  4\n",
      "Label:  5\n",
      "Label:  6\n",
      "Label:  7\n",
      "Label:  8\n",
      "Label:  9\n",
      "Label:  10\n",
      "Label:  0\n",
      "Label:  1\n",
      "Label:  2\n",
      "Label:  3\n",
      "Label:  4\n",
      "Label:  5\n",
      "Label:  6\n",
      "Label:  7\n",
      "Label:  8\n",
      "Label:  9\n",
      "Label:  10\n",
      "Label:  0\n",
      "Label:  1\n",
      "Label:  2\n",
      "Label:  3\n",
      "Label:  4\n",
      "Label:  5\n",
      "Label:  6\n",
      "Label:  7\n",
      "Label:  8\n",
      "Label:  9\n",
      "Label:  10\n",
      "Label:  0\n",
      "Label:  1\n",
      "Label:  2\n",
      "Label:  3\n",
      "Label:  4\n",
      "Label:  5\n",
      "Label:  6\n",
      "Label:  7\n",
      "Label:  8\n",
      "Label:  9\n",
      "Label:  10\n",
      "Label:  0\n",
      "Label:  1\n",
      "Label:  2\n",
      "Label:  3\n",
      "Label:  4\n",
      "Label:  5\n",
      "Label:  6\n",
      "Label:  7\n",
      "Label:  8\n",
      "Label:  9\n",
      "Label:  10\n",
      "Label:  0\n",
      "Label:  1\n",
      "Label:  2\n",
      "Label:  3\n",
      "Label:  4\n",
      "Label:  5\n",
      "Label:  6\n",
      "Label:  7\n",
      "Label:  8\n",
      "Label:  9\n",
      "Label:  10\n",
      "Label:  0\n",
      "Label:  1\n",
      "Label:  2\n",
      "Label:  3\n",
      "Label:  4\n",
      "Label:  5\n",
      "Label:  6\n",
      "Label:  7\n",
      "Label:  8\n",
      "Label:  9\n",
      "Label:  10\n",
      "Label:  0\n",
      "Label:  1\n",
      "Label:  2\n",
      "Label:  3\n",
      "Label:  4\n",
      "Label:  5\n",
      "Label:  6\n",
      "Label:  7\n",
      "Label:  8\n",
      "Label:  9\n",
      "Label:  10\n",
      "Label:  0\n",
      "Label:  1\n",
      "Label:  2\n",
      "Label:  3\n",
      "Label:  4\n",
      "Label:  5\n",
      "Label:  6\n",
      "Label:  7\n",
      "Label:  8\n",
      "Label:  9\n",
      "Label:  10\n",
      "Label:  0\n",
      "Label:  1\n",
      "Label:  2\n",
      "Label:  3\n",
      "Label:  4\n",
      "Label:  5\n",
      "Label:  6\n",
      "Label:  7\n",
      "Label:  8\n",
      "Label:  9\n",
      "Label:  10\n",
      "Label:  0\n",
      "Label:  1\n",
      "Label:  2\n",
      "Label:  3\n",
      "Label:  4\n",
      "Label:  5\n",
      "Label:  6\n",
      "Label:  7\n",
      "Label:  8\n",
      "Label:  9\n",
      "Label:  10\n",
      "Label:  0\n",
      "Label:  1\n",
      "Label:  2\n",
      "Label:  3\n",
      "Label:  4\n",
      "Label:  5\n",
      "Label:  6\n",
      "Label:  7\n",
      "Label:  8\n",
      "Label:  9\n",
      "Label:  10\n",
      "Label:  0\n",
      "Label:  1\n",
      "Label:  2\n",
      "Label:  3\n",
      "Label:  4\n",
      "Label:  5\n",
      "Label:  6\n",
      "Label:  7\n",
      "Label:  8\n",
      "Label:  9\n",
      "Label:  10\n",
      "Label:  0\n",
      "Label:  1\n",
      "Label:  2\n",
      "Label:  3\n",
      "Label:  4\n",
      "Label:  5\n",
      "Label:  6\n",
      "Label:  7\n",
      "Label:  8\n",
      "Label:  9\n",
      "Label:  10\n",
      "Label:  0\n",
      "Label:  1\n",
      "Label:  2\n",
      "Label:  3\n",
      "Label:  4\n",
      "Label:  5\n",
      "Label:  6\n",
      "Label:  7\n",
      "Label:  8\n",
      "Label:  9\n",
      "Label:  10\n",
      "Label:  0\n",
      "Label:  1\n",
      "Label:  2\n",
      "Label:  3\n",
      "Label:  4\n",
      "Label:  5\n",
      "Label:  6\n",
      "Label:  7\n",
      "Label:  8\n",
      "Label:  9\n",
      "Label:  10\n",
      "Label:  0\n",
      "Label:  1\n",
      "Label:  2\n",
      "Label:  3\n",
      "Label:  4\n",
      "Label:  5\n",
      "Label:  6\n",
      "Label:  7\n",
      "Label:  8\n",
      "Label:  9\n",
      "Label:  10\n",
      "Label:  0\n",
      "Label:  1\n",
      "Label:  2\n",
      "Label:  3\n",
      "Label:  4\n",
      "Label:  5\n",
      "Label:  6\n",
      "Label:  7\n",
      "Label:  8\n",
      "Label:  9\n",
      "Label:  10\n",
      "Label:  0\n",
      "Label:  1\n",
      "Label:  2\n",
      "Label:  3\n",
      "Label:  4\n",
      "Label:  5\n",
      "Label:  6\n",
      "Label:  7\n",
      "Label:  8\n",
      "Label:  9\n",
      "Label:  10\n",
      "Label:  0\n",
      "Label:  1\n",
      "Label:  2\n",
      "Label:  3\n",
      "Label:  4\n",
      "Label:  5\n",
      "Label:  6\n",
      "Label:  7\n",
      "Label:  8\n",
      "Label:  9\n",
      "Label:  10\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(graph_torch_data, batch_size=2)\n",
    "\n",
    "for batch in dataloader:\n",
    "    for label in range(num_classes):\n",
    "        check_batch(batch, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 2, 10,  1, 10,  3, 10, 10,  9, 10, 10,  5,  2,  5,  8,  0,  8,  5,  0,\n",
       "          4,  7,  3,  0,  0,  8, 10,  7,  7,  8,  2,  2,  7,  7,  8,  7,  2,  4,\n",
       "          6, 10, 10,  4,  4, 10]),\n",
       " tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 7, 7, 7, 4, 4,\n",
       "         4, 4, 4, 7, 7, 8, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "         4, 4]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_torch_data[0].edge_type, graph_torch_data[1].edge_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([18,  3, 20, 33,  4,  0, 37, 25, 32,  1, 22,  9,  7, 14, 17,  2, 27, 23,\n",
       "         31, 39, 15, 24, 13, 12, 35, 40, 16, 29, 26,  5, 36, 34,  8, 38]),\n",
       " tensor([44,  7,  9, 37, 31,  6, 39,  8, 14, 13, 29,  5, 27,  1, 46, 19, 45, 23,\n",
       "         36, 40, 33, 15,  0, 17, 24,  4, 34, 32, 43, 12, 16, 21, 10, 48, 11, 35,\n",
       "         38, 41,  2,  3]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_torch_data[0].train_edge_mask, graph_torch_data[1].train_edge_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18,  3, 20, 33,  4,  0, 37, 25, 32,  1, 22,  9,  7, 14, 17,  2, 27, 23,\n",
       "        31, 39, 15, 24, 13, 12, 35, 40, 16, 29, 26,  5, 36, 34,  8, 38])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_torch_data[0].train_edge_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4, 10,  3,  7,  3,  2, 10,  7,  8, 10,  0, 10,  9,  0,  0,  1,  8,  8,\n",
       "         7,  4,  8, 10,  8,  5,  4,  4,  5,  2,  7, 10,  6,  2, 10, 10])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_torch_data[0].edge_type[graph_torch_data[0].train_edge_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4, 10,  3,  7,  3,  2, 10,  7,  8, 10,  0, 10,  9,  0,  0,  1,  8,  8,\n",
       "         7,  4,  8, 10,  8,  5,  4,  4,  5,  2,  7, 10,  6,  2, 10, 10])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_torch_data[0].edge_type[graph_torch_data[0].train_edge_mask] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0, 19, 24, 25]),\n",
       " tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 11, 13, 14, 16, 17, 18, 19, 20,\n",
       "         21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g1_ind, g2_ind = (graph_torch_data[0].edge_type[graph_torch_data[0].train_edge_mask] == 4).nonzero(as_tuple=True)[0], (graph_torch_data[1].edge_type[graph_torch_data[1].train_edge_mask] == 4).nonzero(as_tuple=True)[0]\n",
    "g1_ind, g2_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18,  3, 20, 33,  4,  0, 37, 25, 32,  1, 22,  9,  7, 14, 17,  2, 27, 23,\n",
       "        31, 39, 15, 24, 13, 12, 35, 40, 16, 29, 26,  5, 36, 34,  8, 38, 86, 49,\n",
       "        51, 79, 73, 48, 81, 50, 56, 55, 71, 47, 69, 43, 88, 61, 87, 65, 78, 82,\n",
       "        75, 57, 42, 59, 66, 46, 76, 74, 85, 54, 58, 63, 52, 90, 53, 77, 80, 83,\n",
       "        44, 45])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.train_edge_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 19, 24, 25, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 45, 47, 48, 50,\n",
       "        51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69,\n",
       "        70, 71, 72, 73])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_indices = (batch.edge_type[batch.train_edge_mask] == 4).nonzero(as_tuple=True)[0]\n",
    "batch_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4, 10,  3,  7,  3,  2, 10,  7,  8, 10,  0, 10,  9,  0,  0,  1,  8,  8,\n",
       "         7,  4,  8, 10,  8,  5,  4,  4,  5,  2,  7, 10,  6,  2, 10, 10,  4,  4,\n",
       "         4,  4,  4,  4,  4,  4,  4,  4,  8,  4,  7,  4,  4,  7,  4,  4,  4,  4,\n",
       "         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  7,  4,  4,  4,  4,  4,  4,\n",
       "         4,  4])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.edge_type[batch.train_edge_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(768)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(batch.edge_attr[batch.train_edge_mask][1] == batch.edge_attr[batch.train_edge_mask][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(358, 261)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.num_nodes, graph_torch_data[0].num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 60, 358, 295, 149, 321, 181, 154,  62, 260, 345, 381, 394, 101, 264,\n",
       "        378, 361, 288,  89, 122, 175])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_torch_data[0].train_edge_mask[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([38, 33, 20, 21, 19, 18, 26, 24,  1,  7, 29, 36, 12, 37,  6, 11, 41,  5,\n",
       "        14,  9])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_torch_data[1].train_edge_mask[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([291,  37, 121, 372, 240, 327, 362,  17,   9,  29, 179, 271, 371, 254,\n",
       "          7, 123,  70,  47, 162, 302])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.train_edge_mask[261:281]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = getattr(graph_edge_dataset, 'num_edges_type')\n",
    "for batch in dataloader:\n",
    "    check_batch(batch, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.edge_type[146]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([483])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = (batch.edge_type == 3).nonzero(as_tuple=True)[0]\n",
    "indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(28), tensor(874))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices[0], indices[246]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.edge_type[655]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.8145, -0.0470, -0.2188,  0.0849, -0.0718, -0.0343,  0.3368, -0.5850,\n",
       "          0.0766, -0.4512, -1.1710,  0.5025,  0.0528,  0.0865, -0.1269,  0.4233,\n",
       "          1.0132,  0.6537,  0.2685, -1.4494]),\n",
       " tensor([-0.8144, -0.0473, -0.2187,  0.0840, -0.0722, -0.0341,  0.3368, -0.5854,\n",
       "          0.0773, -0.4516, -1.1710,  0.5032,  0.0532,  0.0867, -0.1271,  0.4232,\n",
       "          1.0130,  0.6537,  0.2690, -1.4496]))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.edge_attr[indices[0]][:20], batch.edge_attr[indices[246]][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([483, 768])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_embeddings = batch.edge_attr[indices]\n",
    "selected_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([246, 247, 248, 249, 371, 372, 373])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "comparison = torch.all(selected_embeddings == selected_embeddings[0], dim=1)\n",
    "torch.where(~comparison)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getattr(graph_node_dataset, 'num_nodes_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[63]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(graph_node_dataset, f\"node_exclude_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 2.116132974624634\n",
      "Epoch [2/100], Loss: 1.8642759323120117\n",
      "Epoch [3/100], Loss: 0.9200063347816467\n",
      "Epoch [4/100], Loss: 1.4972515106201172\n",
      "Epoch [5/100], Loss: 1.2084574699401855\n",
      "Epoch [6/100], Loss: 0.6340409517288208\n",
      "Epoch [7/100], Loss: 0.5120918154716492\n",
      "Epoch [8/100], Loss: 0.3598491847515106\n",
      "Epoch [9/100], Loss: 0.46989649534225464\n",
      "Epoch [10/100], Loss: 0.46171605587005615\n",
      "Epoch [11/100], Loss: 0.23111966252326965\n",
      "Epoch [12/100], Loss: 0.05078734830021858\n",
      "Epoch [13/100], Loss: 0.03232596442103386\n",
      "Epoch [14/100], Loss: 0.103300541639328\n",
      "Epoch [15/100], Loss: 0.04028802365064621\n",
      "Epoch [16/100], Loss: 0.07596289366483688\n",
      "Epoch [17/100], Loss: 0.06882505863904953\n",
      "Epoch [18/100], Loss: 0.07486800849437714\n",
      "Epoch [19/100], Loss: 0.024326512590050697\n",
      "Epoch [20/100], Loss: 0.009404868818819523\n",
      "Epoch [21/100], Loss: 0.18503651022911072\n",
      "Epoch [22/100], Loss: 0.058016229420900345\n",
      "Epoch [23/100], Loss: 0.03199069947004318\n",
      "Epoch [24/100], Loss: 0.01217227429151535\n",
      "Epoch [25/100], Loss: 0.01662457548081875\n",
      "Epoch [26/100], Loss: 0.021815884858369827\n",
      "Epoch [27/100], Loss: 0.013024442829191685\n",
      "Epoch [28/100], Loss: 0.011768313124775887\n",
      "Epoch [29/100], Loss: 0.009254739619791508\n",
      "Epoch [30/100], Loss: 0.0051973722875118256\n",
      "Epoch [31/100], Loss: 0.007205064874142408\n",
      "Epoch [32/100], Loss: 0.008335362188518047\n",
      "Epoch [33/100], Loss: 0.013306076638400555\n",
      "Epoch [34/100], Loss: 0.027874495834112167\n",
      "Epoch [35/100], Loss: 0.007356611546128988\n",
      "Epoch [36/100], Loss: 0.002389567904174328\n",
      "Epoch [37/100], Loss: 0.007000482175499201\n",
      "Epoch [38/100], Loss: 0.0040350789204239845\n",
      "Epoch [39/100], Loss: 0.0027190428227186203\n",
      "Epoch [40/100], Loss: 0.007152935955673456\n",
      "Epoch [41/100], Loss: 0.004264167044311762\n",
      "Epoch [42/100], Loss: 0.0037836532574146986\n",
      "Epoch [43/100], Loss: 0.002940859878435731\n",
      "Epoch [44/100], Loss: 0.0009619951015338302\n",
      "Epoch [45/100], Loss: 0.00455621350556612\n",
      "Epoch [46/100], Loss: 0.0015365488361567259\n",
      "Epoch [47/100], Loss: 0.0051601724699139595\n",
      "Epoch [48/100], Loss: 0.004803985822945833\n",
      "Epoch [49/100], Loss: 0.0020907132420688868\n",
      "Epoch [50/100], Loss: 0.0048942239955067635\n",
      "Epoch [51/100], Loss: 0.0036007952876389027\n",
      "Epoch [52/100], Loss: 0.001996150705963373\n",
      "Epoch [53/100], Loss: 0.0025096717290580273\n",
      "Epoch [54/100], Loss: 0.0012072873068973422\n",
      "Epoch [55/100], Loss: 0.0034139195922762156\n",
      "Epoch [56/100], Loss: 0.001270962180569768\n",
      "Epoch [57/100], Loss: 0.0028688996098935604\n",
      "Epoch [58/100], Loss: 0.0013771391240879893\n",
      "Epoch [59/100], Loss: 0.0025942132342606783\n",
      "Epoch [60/100], Loss: 0.000978655181825161\n",
      "Epoch [61/100], Loss: 0.00449697719886899\n",
      "Epoch [62/100], Loss: 0.0011497014202177525\n",
      "Epoch [63/100], Loss: 0.000948023225646466\n",
      "Epoch [64/100], Loss: 0.0013472128193825483\n",
      "Epoch [65/100], Loss: 0.0012831093044951558\n",
      "Epoch [66/100], Loss: 0.001424156129360199\n",
      "Epoch [67/100], Loss: 0.0012191107962280512\n",
      "Epoch [68/100], Loss: 0.007880091667175293\n",
      "Epoch [69/100], Loss: 0.001837543211877346\n",
      "Epoch [70/100], Loss: 0.0028322595171630383\n",
      "Epoch [71/100], Loss: 0.000866979593411088\n",
      "Epoch [72/100], Loss: 0.0012691931333392859\n",
      "Epoch [73/100], Loss: 0.000566300586797297\n",
      "Epoch [74/100], Loss: 0.0028302608989179134\n",
      "Epoch [75/100], Loss: 0.0022211535833775997\n",
      "Epoch [76/100], Loss: 0.0006336088990792632\n",
      "Epoch [77/100], Loss: 0.0026582235004752874\n",
      "Epoch [78/100], Loss: 0.0014077856903895736\n",
      "Epoch [79/100], Loss: 0.0011828362476080656\n",
      "Epoch [80/100], Loss: 0.0013309984933584929\n",
      "Epoch [81/100], Loss: 0.0013446907978504896\n",
      "Epoch [82/100], Loss: 0.003698754357174039\n",
      "Epoch [83/100], Loss: 0.0031467401422560215\n",
      "Epoch [84/100], Loss: 0.0023123244754970074\n",
      "Epoch [85/100], Loss: 0.0007840156322345138\n",
      "Epoch [86/100], Loss: 0.0024416684173047543\n",
      "Epoch [87/100], Loss: 0.0007428807439282537\n",
      "Epoch [88/100], Loss: 0.0009260927326977253\n",
      "Epoch [89/100], Loss: 0.006319513078778982\n",
      "Epoch [90/100], Loss: 0.0006339700776152313\n",
      "Epoch [91/100], Loss: 0.0010267931502312422\n",
      "Epoch [92/100], Loss: 0.001342594507150352\n",
      "Epoch [93/100], Loss: 0.0011782284127548337\n",
      "Epoch [94/100], Loss: 0.0024148887023329735\n",
      "Epoch [95/100], Loss: 0.0010240395786240697\n",
      "Epoch [96/100], Loss: 0.0012994232820346951\n",
      "Epoch [97/100], Loss: 0.0012770653702318668\n",
      "Epoch [98/100], Loss: 0.000690891349222511\n",
      "Epoch [99/100], Loss: 0.0006225196411833167\n",
      "Epoch [100/100], Loss: 8.874274499248713e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the neural network model\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        \n",
    "        # Define layers\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.layer2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.layer3 = nn.Linear(hidden_size2, output_size)\n",
    "        \n",
    "        # Define activation and dropout\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass data through the first layer, apply activation and dropout\n",
    "        # x = torch.cat([x[edge_index[0]], x[edge_index[1]]], dim=1)\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Second layer\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Third layer (output layer)\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Example usage with data X, y\n",
    "exclude_labels = getattr(graph_edge_dataset, f\"edge_exclude_type\")\n",
    "input_size = graph_torch_data[0].edge_attr.shape[1]  # Assuming X is of shape (batch_size, input_features)\n",
    "hidden_size1 = 128\n",
    "hidden_size2 = 64\n",
    "output_size = getattr(graph_edge_dataset, 'num_edges_type')  # Assuming y contains class labels, this will be the number of classes\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleClassifier(input_size, hidden_size1, hidden_size2, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Example training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        train_mask = batch.train_edge_mask\n",
    "        model.train()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch.edge_attr[train_mask])\n",
    "        # pred = outputs[train_mask]\n",
    "        labels = batch.edge_type[train_mask]\n",
    "        \n",
    "        mask = ~torch.isin(labels, torch.tensor(exclude_labels))\n",
    "\n",
    "        loss = criterion(outputs[mask], labels[mask])\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2, 10,  1, 10,  3, 10, 10,  9, 10, 10,  5,  2,  5,  8,  0,  8,  5,  0,\n",
       "         4,  7,  3,  0,  0,  8, 10,  7,  7,  8,  2,  2,  7,  7,  8,  7,  2,  4,\n",
       "         6, 10, 10,  4,  4, 10])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_torch_data[0].edge_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(768)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(graph_torch_data[0].edge_attr[0] == graph_torch_data[0].edge_attr[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch_geometric.nn import aggr\n",
    "from torch_geometric.nn import (\n",
    "    global_add_pool,\n",
    "    global_max_pool,\n",
    "    global_mean_pool,\n",
    ")\n",
    "import torch_geometric\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "aggregation_methods = {\n",
    "    'mean': aggr.MeanAggregation(),\n",
    "    'sum': aggr.SumAggregation(),\n",
    "    'max': aggr.MaxAggregation(),\n",
    "    'mul': aggr.MulAggregation(),\n",
    "}\n",
    "\n",
    "supported_conv_models = {\n",
    "    'GCNConv': False, ## True or False if the model requires num_heads\n",
    "    'GraphConv': False,\n",
    "    'GATConv': True,\n",
    "    'SAGEConv': False,\n",
    "    'GINConv': False,\n",
    "    'GATv2Conv': True,\n",
    "}\n",
    "\n",
    "global_pooling_methods = {\n",
    "    'sum': global_add_pool,\n",
    "    'mean': global_mean_pool,\n",
    "    'max': global_max_pool,\n",
    "}\n",
    "\n",
    "\n",
    "class GNNConv(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        A general GNN model created using the PyTorch Geometric library\n",
    "        model_name: the name of the GNN model\n",
    "        input_dim: the input dimension\n",
    "        hidden_dim: the hidden dimension\n",
    "        out_dim: the output dimension\n",
    "\n",
    "        num_layers: the number of GNN layers\n",
    "        num_heads: the number of heads in the GNN layer\n",
    "        residual: whether to use residual connections\n",
    "        l_norm: whether to use layer normalization\n",
    "        dropout: the dropout probability\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            model_name, \n",
    "            input_dim, \n",
    "            hidden_dim, \n",
    "            out_dim, \n",
    "            num_layers, \n",
    "            num_heads=None, \n",
    "            residual=False, \n",
    "            l_norm=False, \n",
    "            dropout=0.1,\n",
    "            aggregation='mean',\n",
    "            edge_dim=None\n",
    "        ):\n",
    "        super(GNNConv, self).__init__()\n",
    "\n",
    "        assert model_name in supported_conv_models, f\"Model {model_name} not supported. Choose from {supported_conv_models.keys()}\"\n",
    "        heads_supported = supported_conv_models[model_name]\n",
    "        if heads_supported and num_heads is None:\n",
    "            raise ValueError(f\"Model {model_name} requires num_heads to be set to an integer\")\n",
    "        \n",
    "        if not heads_supported and num_heads is not None:\n",
    "            num_heads = None\n",
    "\n",
    "        assert aggregation in aggregation_methods, f\"Aggregation method {aggregation} not supported. Choose from {aggregation_methods.keys()}\"\n",
    "        aggregation = aggregation_methods[aggregation]\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.embed_dim = hidden_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.residual = residual\n",
    "        self.l_norm = l_norm\n",
    "        self.dropout = dropout\n",
    "        self.aggregation = aggregation\n",
    "        self.edge_dim = edge_dim\n",
    "        \n",
    "\n",
    "        gnn_model = getattr(torch_geometric.nn, model_name)\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        if num_heads is None:\n",
    "            input_layer = gnn_model(input_dim, hidden_dim, aggr=aggregation)\n",
    "        else:\n",
    "            input_layer = gnn_model(\n",
    "                input_dim, \n",
    "                hidden_dim, \n",
    "                heads=num_heads, \n",
    "                aggr=aggregation,\n",
    "                edge_dim=edge_dim\n",
    "            )\n",
    "        self.conv_layers.append(input_layer)\n",
    "\n",
    "        for _ in range(num_layers - 2):\n",
    "            if num_heads is None:\n",
    "                conv = gnn_model(hidden_dim, hidden_dim, aggr=aggregation)\n",
    "                self.conv_layers.append(conv)\n",
    "            else:\n",
    "                conv = gnn_model(\n",
    "                    num_heads*hidden_dim, \n",
    "                    hidden_dim, \n",
    "                    heads=num_heads, \n",
    "                    aggr=aggregation,\n",
    "                    edge_dim=edge_dim\n",
    "                )\n",
    "                self.conv_layers.append(conv)\n",
    "\n",
    "        if num_heads is None:\n",
    "            self.conv_layers.append(gnn_model(hidden_dim, out_dim, aggr=aggregation))\n",
    "        else:\n",
    "            self.conv_layers.append(gnn_model(\n",
    "                num_heads*hidden_dim, \n",
    "                out_dim, \n",
    "                heads=num_heads, \n",
    "                aggr=aggregation,\n",
    "                edge_dim=edge_dim\n",
    "            ))\n",
    "            \n",
    "        self.activation = nn.ReLU()\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim if num_heads is None else num_heads*hidden_dim) if l_norm else None\n",
    "        self.residual = residual\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, in_feat, edge_index, edge_attr=None):\n",
    "        \n",
    "        h = in_feat\n",
    "        h = self.conv_layers[0](h, edge_index, edge_attr) if isinstance(edge_attr, torch.Tensor) else self.conv_layers[0](h, edge_index)\n",
    "        h = self.activation(h)\n",
    "        if self.layer_norm is not None:\n",
    "            h = self.layer_norm(h)\n",
    "        h = self.dropout(h)\n",
    "\n",
    "        for conv in self.conv_layers[1:-1]:\n",
    "            nh = conv(h, edge_index, edge_attr) if edge_attr is not None else conv(h, edge_index)\n",
    "            h = nh if not self.residual else nh + h\n",
    "            h = self.activation(h)\n",
    "            if self.layer_norm is not None:\n",
    "                h = self.layer_norm(h)\n",
    "            h = self.dropout(h)\n",
    "        \n",
    "        h = self.conv_layers[-1](h, edge_index)\n",
    "        h = self.activation(h)\n",
    "        if self.layer_norm is not None:\n",
    "            h = self.layer_norm(h)\n",
    "        h = self.dropout(h)\n",
    "        return h\n",
    "  \n",
    "\n",
    "class EdgeClassifer(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    An MLP predictor for link prediction\n",
    "\n",
    "    h_feats: the input dimension\n",
    "    num_classes: the number of classes\n",
    "    num_layers: the number of layers in the MLP\n",
    "\n",
    "    This class concatenates the node embeddings of the two nodes in the edge\n",
    "    The concatenated embeddings are then passed through an MLP\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            input_dim,\n",
    "            hidden_dim, \n",
    "            num_classes,\n",
    "            num_layers=2, \n",
    "            dropout=0.1,\n",
    "            edge_dim=None,\n",
    "            bias=True\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.embed_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # in_feats = input_dim * 2\n",
    "        # if edge_dim is not None:\n",
    "        #     in_feats += edge_dim\n",
    "        \n",
    "        in_feats = edge_dim\n",
    "        \n",
    "        for _ in range(num_layers):\n",
    "            self.layers.append(nn.Linear(in_feats, hidden_dim, bias=bias))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.Dropout(dropout))\n",
    "            in_feats = hidden_dim\n",
    "        \n",
    "        self.layers.append(nn.Linear(hidden_dim, num_classes, bias=bias))\n",
    "        self.layers.append(nn.ReLU())\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        # h = torch.cat([x[edge_index[0]], x[edge_index[1]]], dim=-1)\n",
    "        # if edge_attr is not None:\n",
    "        #     h = torch.cat([h, edge_attr], dim=-1)\n",
    "        \n",
    "        h = edge_attr\n",
    "        for layer in self.layers:\n",
    "            h = layer(h)\n",
    "        \n",
    "        return h\n",
    "    \n",
    "\n",
    "class NodeClassifier(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    An MLP predictor for link prediction\n",
    "\n",
    "    h_feats: the input dimension\n",
    "    num_classes: the number of classes\n",
    "    num_layers: the number of layers in the MLP\n",
    "\n",
    "    This class concatenates the node embeddings of the two nodes in the edge\n",
    "    The concatenated embeddings are then passed through an MLP\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            input_dim,\n",
    "            hidden_dim, \n",
    "            num_classes,\n",
    "            num_layers=2, \n",
    "            dropout=0.3,\n",
    "            bias=True\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.embed_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(nn.Linear(input_dim, hidden_dim, bias=bias))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.Dropout(dropout))\n",
    "            input_dim = hidden_dim\n",
    "        \n",
    "        self.layers.append(nn.Linear(hidden_dim, num_classes, bias=bias))\n",
    "        self.layers.append(nn.ReLU())\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        for layer in self.layers:\n",
    "            h = layer(h)\n",
    "        \n",
    "        return h\n",
    "    \n",
    "\n",
    "class GraphClassifer(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    An MLP predictor for link prediction\n",
    "\n",
    "    h_feats: the input dimension\n",
    "    num_classes: the number of classes\n",
    "    num_layers: the number of layers in the MLP\n",
    "\n",
    "    This class concatenates the node embeddings of the two nodes in the edge\n",
    "    The concatenated embeddings are then passed through an MLP\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            input_dim, \n",
    "            num_classes,\n",
    "            global_pool='mean',\n",
    "            bias=False\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.layers.append(nn.Linear(input_dim, num_classes, bias=bias))\n",
    "        self.global_pool = global_pooling_methods[global_pool]\n",
    "\n",
    "    def forward(self, x, batch):\n",
    "        h = self.global_pool(x, batch)\n",
    "        for layer in self.layers:\n",
    "            h = layer(h)\n",
    "        \n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(GNNConv(\n",
       "   (aggregation): MeanAggregation()\n",
       "   (conv_layers): ModuleList(\n",
       "     (0): GATv2Conv(768, 128, heads=2)\n",
       "     (1-2): 2 x GATv2Conv(256, 128, heads=2)\n",
       "   )\n",
       "   (activation): ReLU()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " EdgeClassifer(\n",
       "   (layers): ModuleList(\n",
       "     (0): Linear(in_features=768, out_features=128, bias=False)\n",
       "     (1): ReLU()\n",
       "     (2): Dropout(p=0.1, inplace=False)\n",
       "     (3): Linear(in_features=128, out_features=11, bias=False)\n",
       "     (4): ReLU()\n",
       "   )\n",
       " ),\n",
       " NodeClassifier(\n",
       "   (layers): ModuleList(\n",
       "     (0): Linear(in_features=128, out_features=12, bias=False)\n",
       "     (1): ReLU()\n",
       "   )\n",
       " ))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "input_dim = graph_torch_data[0].x.shape[1]\n",
    "\n",
    "\n",
    "model_name = 'GATv2Conv'\n",
    "\n",
    "hidden_dim = 128\n",
    "output_dim = 128\n",
    "num_conv_layers = 3\n",
    "num_mlp_layers = 1\n",
    "num_heads = 2\n",
    "residual = False\n",
    "l_norm = False\n",
    "dropout = 0.1\n",
    "aggregation = 'mean'\n",
    "\n",
    "num_edges_label = f\"num_edges_type\"\n",
    "num_nodes_label = f\"num_nodes_type\"\n",
    "assert hasattr(graph_edge_dataset, num_edges_label), f\"Graph dataset does not have attribute {num_edges_label}\"\n",
    "num_edge_classes = getattr(graph_edge_dataset, num_edges_label)\n",
    "# assert hasattr(graph_node_dataset, num_nodes_label), f\"Graph dataset does not have attribute {num_nodes_label}\"\n",
    "# num_node_classes = getattr(graph_node_dataset, num_nodes_label)\n",
    "\n",
    "\n",
    "edge_dim = graph_edge_dataset[0].data.edge_attr.shape[1]\n",
    "\n",
    "logs_dir = os.path.join(\"logs\")\n",
    "use_edge_attrs = True\n",
    "\n",
    "gnn_conv_model = GNNConv(\n",
    "    model_name=model_name,\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    out_dim=output_dim,\n",
    "    num_layers=num_conv_layers,\n",
    "    num_heads=num_heads,\n",
    "    residual=residual,\n",
    "    l_norm=l_norm,\n",
    "    dropout=dropout,\n",
    "    aggregation=aggregation,\n",
    "    edge_dim=edge_dim\n",
    ")\n",
    "\n",
    "clf_input_dim = output_dim\n",
    "edge_mlp_predictor = EdgeClassifer(\n",
    "    input_dim=clf_input_dim if not num_heads else num_heads*clf_input_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_layers=num_mlp_layers, \n",
    "    num_classes=num_edge_classes,\n",
    "    edge_dim=edge_dim if use_edge_attrs else None,\n",
    "    bias=False,\n",
    ")\n",
    "\n",
    "node_mlp_predictor = NodeClassifier(\n",
    "    input_dim=clf_input_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_layers=num_mlp_layers, \n",
    "    num_classes=12,\n",
    "    bias=False,\n",
    ")\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, torch.nn.Linear) or isinstance(m, torch.nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "gnn_conv_model, edge_mlp_predictor, node_mlp_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import device\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from itertools import chain\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "gnn_conv_model.apply(init_weights)\n",
    "node_mlp_predictor.apply(init_weights)\n",
    "edge_mlp_predictor.apply(init_weights)\n",
    "\n",
    "cls_label = 'type'\n",
    "num_epochs = 300\n",
    "\n",
    "optimizer = Adam(chain(gnn_conv_model.parameters(), node_mlp_predictor.parameters()), lr=1e-3)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "edge2index = lambda g: torch.stack(list(g.edges())).contiguous()\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    graph_torch_data, \n",
    "    batch_size=16, \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7fa50e5ce7d0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.metrics import f1_score, balanced_accuracy_score, precision_score, recall_score\n",
    "import torch\n",
    "\n",
    "\n",
    "def compute_classification_metrics(preds, labels):\n",
    "    \"\"\"\n",
    "    Compute F1-score, balanced accuracy, precision, and recall for multi-class classification.\n",
    "    \n",
    "    Args:\n",
    "        preds (torch.Tensor): Predictions from the model (logits or probabilities). Shape: [num_samples, num_classes]\n",
    "        labels (torch.Tensor): Ground truth labels. Shape: [num_samples]\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing metrics (F1-score, balanced accuracy, precision, recall).\n",
    "    \"\"\"\n",
    "    # Convert predictions to class labels\n",
    "    preds = torch.argmax(preds, dim=1).cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    # F1-score (macro and weighted)\n",
    "    metrics['f1_macro'] = f1_score(labels, preds, average='macro')\n",
    "    metrics['f1_weighted'] = f1_score(labels, preds, average='weighted')\n",
    "\n",
    "    # Balanced Accuracy\n",
    "    metrics['balanced_accuracy'] = balanced_accuracy_score(labels, preds)\n",
    "\n",
    "    # Precision (macro and weighted)\n",
    "    metrics['precision_macro'] = precision_score(labels, preds, average='macro', zero_division=0)\n",
    "    metrics['precision_weighted'] = precision_score(labels, preds, average='weighted', zero_division=0)\n",
    "\n",
    "    # Recall (macro and weighted)\n",
    "    metrics['recall_macro'] = recall_score(labels, preds, average='macro', zero_division=0)\n",
    "    metrics['recall_weighted'] = recall_score(labels, preds, average='weighted', zero_division=0)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def get_logits(model, x, edge_index, edge_attr=None):\n",
    "    if edge_attr is not None:\n",
    "        edge_attr = edge_attr.to(device)\n",
    "        h = model(\n",
    "            x.to(device), \n",
    "            edge_index.to(device), \n",
    "            edge_attr.to(device)\n",
    "        )\n",
    "    else:\n",
    "        h = model(\n",
    "            x.to(device), \n",
    "            edge_index.to(device)\n",
    "        )\n",
    "    return h\n",
    "\n",
    "\n",
    "def get_prediction_score(predictor, h, edge_index=None, edge_attr=None):\n",
    "    if edge_attr is not None:\n",
    "        prediction_score = predictor(\n",
    "            h.to(device), \n",
    "            edge_index.to(device), \n",
    "            edge_attr.to(device)\n",
    "        )\n",
    "    elif edge_index is not None and not isinstance(predictor, NodeClassifier):\n",
    "        prediction_score = predictor(\n",
    "            h.to(device), \n",
    "            edge_index.to(device)\n",
    "        )\n",
    "    else:\n",
    "        prediction_score = predictor(h.to(device))\n",
    "    return prediction_score\n",
    "\n",
    "\n",
    "def train(model, predictor):\n",
    "    exclude_labels = getattr(graph_edge_dataset, f\"edge_exclude_type\")\n",
    "    model.train()\n",
    "    predictor.train()\n",
    "    all_preds, all_labels = list(), list()\n",
    "    epoch_loss = 0\n",
    "    epoch_metrics = defaultdict(float)\n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        gnn_conv_model.zero_grad()\n",
    "        predictor.zero_grad()\n",
    "        x = data.x\n",
    "        edge_index =  data.edge_index\n",
    "        train_mask = data.train_edge_mask\n",
    "        edge_attr = data.edge_attr[train_mask] if use_edge_attrs else None\n",
    "        \n",
    "        h = get_logits(model, x, edge_index, edge_attr)\n",
    "        scores = get_prediction_score(predictor, h, edge_index, edge_attr)[train_mask]\n",
    "        labels = getattr(data, f\"edge_{cls_label}\")[train_mask]\n",
    "\n",
    "        mask = ~torch.isin(labels, torch.tensor(exclude_labels))\n",
    "        loss = criterion(scores[mask], labels[mask].to(device))\n",
    "        all_preds.append(scores[mask].detach().cpu())\n",
    "        all_labels.append(labels[mask])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "\n",
    "    \n",
    "    all_preds = torch.cat(all_preds, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    epoch_metrics = compute_classification_metrics(all_preds, all_labels)\n",
    "    epoch_metrics['loss'] = epoch_loss        \n",
    "    epoch_metrics['phase'] = 'train'\n",
    "\n",
    "    # print(f\"Train Loss: {epoch_loss}\")\n",
    "    print(f\"Train Metrics: {epoch_metrics}\")\n",
    "\n",
    "    return epoch_metrics\n",
    "\n",
    "\n",
    "def test(model, predictor):\n",
    "    exclude_labels = getattr(graph_edge_dataset, f\"node_exclude_type\")\n",
    "    model.eval()\n",
    "    predictor.eval()\n",
    "    all_preds, all_labels = list(), list()\n",
    "    with torch.no_grad():\n",
    "        epoch_loss = 0\n",
    "        epoch_metrics = defaultdict(float)\n",
    "        for data in dataloader:\n",
    "            x = data.x\n",
    "            edge_index =  data.edge_index\n",
    "            test_mask = data.test_edge_mask\n",
    "            train_mask = data.train_edge_mask\n",
    "            edge_attr = data.edge_attr[train_mask] if use_edge_attrs else None\n",
    "            \n",
    "            h = get_logits(model, x, edge_index, edge_attr)\n",
    "\n",
    "            scores = get_prediction_score(predictor, h, edge_index, edge_attr)[test_mask]\n",
    "            labels = getattr(data, f\"edge_{cls_label}\")[test_mask]\n",
    "\n",
    "            mask = ~torch.isin(labels, torch.tensor(exclude_labels))\n",
    "\n",
    "            all_preds.append(scores[mask].detach().cpu())\n",
    "            all_labels.append(labels[mask])\n",
    "            loss = criterion(scores[mask], labels[mask].to(device))\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_labels = torch.cat(all_labels, dim=0)\n",
    "        epoch_metrics = compute_classification_metrics(all_preds, all_labels)\n",
    "        \n",
    "        epoch_metrics['loss'] = epoch_loss\n",
    "        epoch_metrics['phase'] = 'test'\n",
    "        # print(f\"Epoch Test Loss: {epoch_loss}\\nTest Accuracy: {epoch_acc}\\nTest F1: {epoch_f1}\")\n",
    "\n",
    "        # print(f\"Test Epoch: {epoch_metrics}\")\n",
    "    \n",
    "    return epoch_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EdgeClassifer(\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=768, out_features=128, bias=False)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=128, out_features=11, bias=False)\n",
       "    (4): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "gnn_conv_model.to(device)\n",
    "node_mlp_predictor.to(device)\n",
    "edge_mlp_predictor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e38d39f35a042d6975aa6ce71562f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epochs:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "index 1138 is out of bounds for dimension 0 with size 1104",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m         train_metrics \u001b[38;5;241m=\u001b[39m train(model, predictor)\n\u001b[1;32m      7\u001b[0m         test_metrics \u001b[38;5;241m=\u001b[39m test(model, predictor)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mrun_epochs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgnn_conv_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_mlp_predictor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m, in \u001b[0;36mrun_epochs\u001b[0;34m(model, predictor, num_epochs)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_epochs\u001b[39m(model, predictor, num_epochs):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(num_epochs), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning Epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m         train_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m         test_metrics \u001b[38;5;241m=\u001b[39m test(model, predictor)\n",
      "Cell \u001b[0;32mIn[9], line 92\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, predictor)\u001b[0m\n\u001b[1;32m     89\u001b[0m edge_attr \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39medge_attr[train_mask] \u001b[38;5;28;01mif\u001b[39;00m use_edge_attrs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     91\u001b[0m h \u001b[38;5;241m=\u001b[39m get_logits(model, x, edge_index, edge_attr)\n\u001b[0;32m---> 92\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mget_prediction_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_mask\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     93\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(data, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medge_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_label\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)[train_mask]\n\u001b[1;32m     95\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39mtorch\u001b[38;5;241m.\u001b[39misin(labels, torch\u001b[38;5;241m.\u001b[39mtensor(exclude_labels))\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1138 is out of bounds for dimension 0 with size 1104"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def run_epochs(model, predictor, num_epochs):\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Running Epochs\"):\n",
    "        train_metrics = train(model, predictor)\n",
    "        test_metrics = test(model, predictor)\n",
    "        \n",
    "\n",
    "run_epochs(gnn_conv_model, edge_mlp_predictor, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GNNModel(\n",
       "  (conv_layers): ModuleList(\n",
       "    (0): GATv2Conv(768, 128, heads=2)\n",
       "    (1): GATv2Conv(256, 128, heads=2)\n",
       "    (2): GATv2Conv(256, 128, heads=1)\n",
       "  )\n",
       "  (activation): ReLU()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GNNModel(torch.nn.Module):\n",
    "    \"\"\"GraphSage Network\"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name, \n",
    "        input_dim, \n",
    "        hidden_dim, \n",
    "        out_dim, \n",
    "        num_layers, \n",
    "        num_heads=None, \n",
    "        residual=False, \n",
    "        l_norm=False, \n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super(GNNModel, self).__init__()\n",
    "        gnn_model = getattr(torch_geometric.nn, model_name)\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        if model_name == 'GINConv':\n",
    "            input_layer = gnn_model(nn.Sequential(nn.Linear(input_dim, hidden_dim), nn.ReLU()), train_eps=True)\n",
    "        elif num_heads is None:\n",
    "            input_layer = gnn_model(input_dim, hidden_dim, aggr='SumAggregation')\n",
    "        else:\n",
    "            input_layer = gnn_model(input_dim, hidden_dim, heads=num_heads, aggr='SumAggregation')\n",
    "        self.conv_layers.append(input_layer)\n",
    "\n",
    "        for _ in range(num_layers - 2):\n",
    "            if model_name == 'GINConv':\n",
    "                self.conv_layers.append(gnn_model(nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.ReLU()), train_eps=True))\n",
    "            elif num_heads is None:\n",
    "                self.conv_layers.append(gnn_model(hidden_dim, hidden_dim, aggr='SumAggregation'))\n",
    "            else:\n",
    "                self.conv_layers.append(gnn_model(num_heads*hidden_dim, hidden_dim, heads=num_heads, aggr='SumAggregation'))\n",
    "\n",
    "        if model_name == 'GINConv':\n",
    "            self.conv_layers.append(gnn_model(nn.Sequential(nn.Linear(hidden_dim, out_dim), nn.ReLU()), train_eps=True))\n",
    "        else:\n",
    "            self.conv_layers.append(gnn_model(hidden_dim if num_heads is None else num_heads*hidden_dim, out_dim, aggr='SumAggregation'))\n",
    "            \n",
    "        self.activation = nn.ReLU()\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim if num_heads is None else num_heads*hidden_dim) if l_norm else None\n",
    "        self.residual = residual\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, in_feat, edge_index, edge_attr=None):\n",
    "        def ff(layer, h, edge_index, edge_attr):\n",
    "            if isinstance(edge_attr, torch.Tensor):\n",
    "                return layer(h, edge_index, edge_attr)\n",
    "            return layer(h, edge_index)\n",
    "        \n",
    "        h = in_feat\n",
    "        h = ff(self.conv_layers[0], h, edge_index, edge_attr)\n",
    "        h = self.activation(h)\n",
    "        if self.layer_norm is not None:\n",
    "            h = self.layer_norm(h)\n",
    "        h = self.dropout(h)\n",
    "\n",
    "        for conv in self.conv_layers[1:-1]:\n",
    "            h = ff(conv, h, edge_index, edge_attr) if not self.residual else ff(conv, h, edge_index, edge_attr) + h\n",
    "            h = self.activation(h)\n",
    "            if self.layer_norm is not None:\n",
    "                h = self.layer_norm(h)\n",
    "            h = self.dropout(h)\n",
    "        \n",
    "        h = ff(self.conv_layers[-1], h, edge_index, edge_attr)\n",
    "        return h\n",
    "\n",
    "gnn_model = GNNModel(\n",
    "    model_name=model_name,\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    out_dim=output_dim,\n",
    "    num_layers=num_conv_layers,\n",
    "    num_heads=num_heads,\n",
    "    residual=residual,\n",
    "    l_norm=l_norm,\n",
    "    dropout=dropout\n",
    ")\n",
    "\n",
    "gnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphTrainer:\n",
    "    def __init__(self, model, args) -> None:\n",
    "        self.model = model\n",
    "        self.model.to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=args.gnn_lr)\n",
    "        # self.scheduler = get_linear_schedule_with_warmup(\n",
    "        #     self.optimizer, num_warmup_steps=10, num_training_steps=args.num_training_steps)\n",
    "        self.criteria = nn.CrossEntropyLoss()\n",
    "        self.edge2index = lambda g: torch.stack(list(g.edges())).contiguous()\n",
    "        self.args = args\n",
    "        print(\"GenericGraphTrainer initialized.\")\n",
    "\n",
    "    def run_epoch(self, dataloader, training=True):\n",
    "        torch.set_grad_enabled(training)\n",
    "        if training:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "\n",
    "        train_preds, train_acts = list(), list()\n",
    "        test_preds, test_acts = list(), list()\n",
    "        train_avg_loss, test_avg_loss = 0, 0\n",
    "        _, topk_test_preds = list(), list()\n",
    "\n",
    "        # for batch in tqdm(dataloader, total=len(dataloader), desc=f\"{train_str} GNN\"):\n",
    "        for batch in dataloader:\n",
    "            if training:\n",
    "                self.optimizer.zero_grad()\n",
    "                self.model.zero_grad()\n",
    "            # input_ids, labels = batch\n",
    "            if isinstance(batch, tuple):\n",
    "                batch = batch[0]\n",
    "\n",
    "            labels = batch.ndata['labels'].to(device)\n",
    "            train_mask = batch.ndata['train_mask']\n",
    "            test_mask = batch.ndata['test_mask']\n",
    "\n",
    "            # Forward\n",
    "            # logits = model(batched_graph, features)\n",
    "            logits = self.get_logits(batch)\n",
    "\n",
    "            # Compute prediction\n",
    "            pred = logits.argmax(1)\n",
    "            if logits[train_mask].shape[0] == 0:\n",
    "                continue\n",
    "\n",
    "            # Compute loss for training set nodes\n",
    "            loss = self.criteria(logits[train_mask], labels[train_mask])\n",
    "            train_avg_loss += loss.item()\n",
    "            train_preds += pred[train_mask].tolist()\n",
    "            train_acts += labels[train_mask].tolist()\n",
    "\n",
    "            topk_test_preds += logits[test_mask].topk(min(8, logits.shape[-1])).indices.tolist()\n",
    "\n",
    "            # Backward\n",
    "            if training:\n",
    "                loss.backward(torch.ones_like(loss))\n",
    "                self.optimizer.step()\n",
    "                # self.scheduler.step()\n",
    "\n",
    "            if logits[test_mask].shape[0] == 0:\n",
    "                continue\n",
    "            # Compute loss for test set nodes\n",
    "            test_loss = self.criteria(logits[test_mask], labels[test_mask])\n",
    "            test_avg_loss += test_loss.item()\n",
    "            test_preds += pred[test_mask].tolist()\n",
    "            test_acts += labels[test_mask].tolist()\n",
    "\n",
    "        \n",
    "        train_acc = (np.array(train_preds) == np.array(train_acts)).mean()\n",
    "        test_acc = (np.array(test_preds) == np.array(test_acts)).mean()\n",
    "\n",
    "        # train_predictions_distribution = data_utils.get_predictions_distribution(train_preds, train_acts)\n",
    "        # test_predictions_distribution = data_utils.get_predictions_distribution(test_preds, test_acts)\n",
    "\n",
    "        train_avg_loss = np.log2(1 + train_avg_loss)\n",
    "        test_avg_loss = np.log2(1 + test_avg_loss)\n",
    "        torch.set_grad_enabled(True)\n",
    "        output = {\n",
    "            'train_acc': train_acc, \n",
    "            'train_loss': train_avg_loss,\n",
    "            'test_acc': test_acc,\n",
    "            'test_loss': test_avg_loss,\n",
    "            # 'train_predictions_distribution': train_predictions_distribution,\n",
    "            # 'test_predictions_distribution': test_predictions_distribution\n",
    "            'top2acc': np.mean([1 if label in topk[:2] else 0 for topk, label in zip(topk_test_preds, test_acts)]),\n",
    "            'top3acc': np.mean([1 if label in topk[:3] else 0 for topk, label in zip(topk_test_preds, test_acts)]),\n",
    "            'top4acc': np.mean([1 if label in topk[:4] else 0 for topk, label in zip(topk_test_preds, test_acts)]),\n",
    "            'top5acc': np.mean([1 if label in topk[:5] else 0 for topk, label in zip(topk_test_preds, test_acts)]),\n",
    "            'top6acc': np.mean([1 if label in topk[:6] else 0 for topk, label in zip(topk_test_preds, test_acts)]),\n",
    "        }\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_logits(self, batched_graph):\n",
    "        self.model.zero_grad()\n",
    "        x = batched_graph.ndata['feat'].to(device)\n",
    "        model_name = type(self.model).__name__\n",
    "        if \"GNN\" in model_name:\n",
    "            edge_index = self.edge2index(batched_graph).to(device)\n",
    "            x = x.float()\n",
    "            logits = self.model(x, edge_index)\n",
    "        else:\n",
    "            logits = self.model(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "    def run_epochs(self, dataloader, num_epochs):\n",
    "        max_val_acc, max_train_acc = 0, 0\n",
    "        outputs = list()\n",
    "        for _ in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "        # for epoch in range(num_epochs):\n",
    "            output = self.run_epoch(dataloader, training=True)\n",
    "            \n",
    "            train_acc, test_acc = output['train_acc'], output['test_acc']\n",
    "            max_val_acc = max(max_val_acc, test_acc)\n",
    "            max_train_acc = max(max_train_acc, train_acc)\n",
    "\n",
    "            # if epoch % 50 == 0 and epoch > 0:\n",
    "            # print(f\"Epoch {epoch}: Train loss {output['train_loss']} and Test Loss {output['test_loss']}\")\n",
    "            # print(f\"Train Accuracy: {train_acc} Test Accuracy: {test_acc}\")\n",
    "            outputs.append(output)\n",
    "        \n",
    "        print(f\"Max Test Accuracy: {max_val_acc}\")\n",
    "        print(f\"Max Train Accuracy: {max_train_acc}\")\n",
    "        max_output = max(outputs, key=lambda x: x['test_acc'])\n",
    "        return max_output\n",
    "        \n",
    "\n",
    "    def validate(self, dataloader):\n",
    "        output = self.run_epoch(dataloader, training=False)\n",
    "        train_acc, test_acc = output['train_acc'], output['test_acc']\n",
    "        print(f\"Train loss {output['train_loss']} and Test Loss {output['test_loss']}\")\n",
    "        print(f\"Train Accuracy: {train_acc} Test Accuracy: {test_acc}\")\n",
    "        return output\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
