{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "datasets_dir = 'datasets'\n",
    "ecore_json_path = os.path.join(datasets_dir, 'ecore_555/ecore_555.jsonl')\n",
    "mar_json_path = os.path.join(datasets_dir, 'mar-ecore-github/ecore-github.jsonl')\n",
    "modelsets_uml_json_path = os.path.join(datasets_dir, 'modelset/uml.jsonl')\n",
    "modelsets_ecore_json_path = os.path.join(datasets_dir, 'modelset/ecore.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ecore_555 from pickle\n",
      "Loaded ecore_555 with 281 graphs\n",
      "Loaded ecore_555 with 281 graphs\n",
      "Graphs: 281\n",
      "Loading modelset from pickle\n",
      "Loaded modelset with 830 graphs\n",
      "Loaded modelset with 830 graphs\n",
      "Graphs: 830\n",
      "Loading mar-ecore-github from pickle\n",
      "Loaded mar-ecore-github with 5389 graphs\n",
      "Loaded mar-ecore-github with 5389 graphs\n",
      "Graphs: 5389\n"
     ]
    }
   ],
   "source": [
    "from data_loading.data import ModelDataset\n",
    "\n",
    "config_params = dict(\n",
    "    timeout = 120,\n",
    "    min_enr = 1.2,\n",
    "    min_edges = 10\n",
    ")\n",
    "ecore = ModelDataset('ecore_555', reload=False, **config_params)\n",
    "modelset = ModelDataset('modelset', reload=False, remove_duplicates=True, **config_params)\n",
    "mar = ModelDataset('mar-ecore-github', reload=False, **config_params)\n",
    "\n",
    "\n",
    "datasets = {\n",
    "    'ecore': ecore,\n",
    "    'modelset': modelset,\n",
    "    'mar': mar\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bf45dbfcd044013b99bf20ff0641a80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing ecore_555:   0%|          | 0/281 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from data_loading.graph_dataset import GraphDataset\n",
    "\n",
    "graph_data_params = dict(\n",
    "    distance=2,\n",
    "    reload=True\n",
    ")\n",
    "\n",
    "ecore_graph_dataset = GraphDataset(ecore, **graph_data_params)\n",
    "# modelset_graph_dataset = GraphDataset(modelset, **graph_data_params)\n",
    "# mar_graph_dataset = GraphDataset(mar, **graph_data_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124, torch.Size([2, 100]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "graph, edge_index = pickle.load(open('subgraph.pkl', 'rb'))\n",
    "graph.number_of_edges(), edge_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "ei = torch.tensor(list(graph.numbered_graph.edges)).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 100)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "n = nx.DiGraph()\n",
    "n.add_edges_from([(u, v, graph.numbered_graph.edges[u, v]) for u, v in edge_index.t().tolist()])\n",
    "for node, data in n.nodes(data=True):\n",
    "    data = graph.numbered_graph.nodes[node]\n",
    "    n.nodes[node].update(data)\n",
    "\n",
    "n.number_of_nodes(), n.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 17)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecore_graph = [\n",
    "    g\n",
    "    for g in ecore.graphs if g.number_of_nodes() >= 12 and g.number_of_edges() >= 12 and g.number_of_nodes() <= 20 and g.number_of_edges() <= 20\n",
    "\n",
    "][0]\n",
    "ecore_graph.number_of_nodes(), ecore_graph.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecore_graph.nodes(), ecore_graph.edges(data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from lang2graph.uml import SUPERTYPE, EcoreNxG\n",
    "\n",
    "def create_graph_from_edge_index(G: EcoreNxG, edge_index):\n",
    "    \"\"\"\n",
    "    Create a subgraph from G using only the edges specified in edge_index.\n",
    "    \n",
    "    Parameters:\n",
    "    G (networkx.Graph): The original graph.\n",
    "    edge_index (torch.Tensor): A tensor containing edge indices.\n",
    "    \n",
    "    Returns:\n",
    "    networkx.Graph: A subgraph of G containing only the edges in edge_index.\n",
    "    \"\"\"\n",
    "    # Create a new graph\n",
    "    subgraph = nx.Graph()\n",
    "\n",
    "    # Add nodes and edges from the edge_index to the subgraph\n",
    "    for i in range(edge_index.size(1)):\n",
    "        u = edge_index[0, i].item()\n",
    "        v = edge_index[1, i].item()\n",
    "        u_str, v_str = G.id2label[u], G.id2label[v]\n",
    "        if G.numbered_graph.has_edge(u, v):\n",
    "            subgraph.add_edge(u, v, **G.edges[u_str, v_str])\n",
    "\n",
    "    subgraph.label2id = G.label2id\n",
    "        \n",
    "    subgraph.id2label = G.id2label\n",
    "    return subgraph\n",
    "\n",
    "\n",
    "def get_node_texts(graph: nx.DiGraph, h: int):\n",
    "    \"\"\"\n",
    "    Create node string for each node n in a graph using neighbors of n up to h hops.\n",
    "    \n",
    "    Parameters:\n",
    "    G (networkx.Graph): The graph.\n",
    "    h (int): The number of hops.\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary where keys are nodes and values are node strings.\n",
    "    \"\"\"\n",
    "    node_texts = {}\n",
    "\n",
    "    for node in graph.nodes():\n",
    "        node_str = f\"{node}\"\n",
    "        current_level_nodes = {node}\n",
    "        all_visited_nodes = {node}\n",
    "\n",
    "        for _ in range(1, h + 1):\n",
    "            next_level_nodes = set()\n",
    "            for n in current_level_nodes:\n",
    "                neighbors = set(graph.neighbors(n))\n",
    "                next_level_nodes.update(neighbors - all_visited_nodes)\n",
    "            all_visited_nodes.update(next_level_nodes)\n",
    "            if next_level_nodes:\n",
    "                node_strs = [graph.id2label[i] for i in sorted(next_level_nodes)]\n",
    "                node_str += f\" -> {', '.join(map(str, node_strs))}\"\n",
    "            current_level_nodes = next_level_nodes\n",
    "\n",
    "        node_texts[node] = node_str\n",
    "\n",
    "    return node_texts\n",
    "\n",
    "\n",
    "def get_edge_type(edge_data):\n",
    "\n",
    "    # Reference = 0\n",
    "    # Containment = 1\n",
    "    # Supertype = 2\n",
    "\n",
    "    if edge_data['type'] == SUPERTYPE:\n",
    "        return 2\n",
    "    if edge_data['containment']:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def get_edge_texts(graph: nx.DiGraph):\n",
    "    \"\"\"\n",
    "    Create edge string for each edge in a graph.\n",
    "    \n",
    "    Parameters:\n",
    "    G (networkx.Graph): The graph.\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary where keys are edges and values are edge strings.\n",
    "    \"\"\"\n",
    "    edge_texts = {}\n",
    "\n",
    "    for u, v, data in graph.edges(data=True):\n",
    "        edge_texts[(u, v)] = f\"{graph.id2label[u]} - {get_edge_type(data)} - {graph.id2label[v]}\"\n",
    "\n",
    "    return edge_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "transform = RandomLinkSplit(\n",
    "    num_val=0, \n",
    "    num_test=0.2, \n",
    "    is_undirected=False, \n",
    "    split_labels=True,\n",
    "    neg_sampling_ratio=1,\n",
    "    add_negative_train_samples=True\n",
    ")\n",
    "\n",
    "# Apply the transform\n",
    "train_data, _, test_data = transform(Data(edge_index=ecore_graph.edge_index, num_nodes=ecore_graph.number_of_nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 14]) torch.Size([2, 3])\n",
      "torch.Size([2, 14]) torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(train_data.pos_edge_label_index.shape, test_data.pos_edge_label_index.shape)\n",
    "print(train_data.neg_edge_label_index.shape, test_data.neg_edge_label_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subgraph = create_graph_from_edge_index(ecore_graph, train_data.edge_index)\n",
    "print(train_subgraph.edges(data=True))\n",
    "node_texts = get_node_texts(train_subgraph, 1)  # Get node texts up to 2 hops\n",
    "for node, text in node_texts.items():\n",
    "    print(node, text)\n",
    "\n",
    "edge_texts = get_edge_texts(train_subgraph)\n",
    "for edge, text in edge_texts.items():\n",
    "    print(edge, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embeddings.bert import BertEmbedder\n",
    "\n",
    "embedder = BertEmbedder(model_name='bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.6956, Test Accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "# from torch_geometric.data import Data\n",
    "# from torch_geometric.loader import DataLoader\n",
    "# from torch_geometric.nn import GCNConv\n",
    "# from torch_geometric.nn.aggr import SortAggregation\n",
    "# import networkx as nx\n",
    "# from torch_geometric.transforms import RandomLinkSplit\n",
    "\n",
    "\n",
    "# def remap_node_indices(subgraph, center_node):\n",
    "#     mapping = {node: i for i, node in enumerate(subgraph.nodes())}\n",
    "#     subgraph = nx.relabel_nodes(subgraph, mapping)\n",
    "#     sub_edge_index = torch.tensor(list(subgraph.edges)).t().contiguous()\n",
    "#     sub_x = torch.ones(subgraph.number_of_nodes(), 1)  # Example node features\n",
    "#     center_node_idx = mapping[center_node]\n",
    "#     return sub_x, sub_edge_index, center_node_idx\n",
    "\n",
    "# # Prepare the train and test datasets for SEAL model\n",
    "# class SEALGraphData:\n",
    "#     def __init__(\n",
    "#             self, \n",
    "#             graph,\n",
    "#             edge_index,\n",
    "#             pos_edge_index,\n",
    "#             neg_edge_index,\n",
    "#             hops=1\n",
    "#         ):\n",
    "#         self.edge_index = edge_index\n",
    "#         self.pos_edge_index = pos_edge_index\n",
    "#         self.neg_edge_index = neg_edge_index\n",
    "#         self.graph = graph\n",
    "#         self.hops = hops\n",
    "\n",
    "\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.pos_edge_index.size(1) + self.neg_edge_index.size(1)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         if idx < self.pos_edge_index.size(1):\n",
    "#             u, v = self.pos_edge_index[:, idx]\n",
    "#             y = 1\n",
    "#         else:\n",
    "#             u, v = self.neg_edge_index[:, idx - self.pos_edge_index.size(1)]\n",
    "#             y = 0\n",
    "\n",
    "#         subgraph = nx.ego_graph(self.graph, u.item(), radius=self.hops)\n",
    "#         subgraph = nx.subgraph(subgraph, list(subgraph.nodes) + [v.item()])\n",
    "#         sub_x, sub_edge_index, center_node_idx = remap_node_indices(subgraph, u.item())\n",
    "\n",
    "#         return Data(\n",
    "#             x=sub_x, \n",
    "#             edge_index=sub_edge_index, \n",
    "#             y=y, \n",
    "#             center_node_idx=center_node_idx\n",
    "#         )\n",
    "\n",
    "\n",
    "# def get_link_prediction_train_test_graph_data(\n",
    "#         graph, \n",
    "#         num_val=0, \n",
    "#         num_test=0.2, \n",
    "#         add_negative_train_samples=True,\n",
    "#         neg_sampling_ratio=1,\n",
    "#     ):\n",
    "#     transform = RandomLinkSplit(\n",
    "#         num_val=num_val, \n",
    "#         num_test=num_test, \n",
    "#         neg_sampling_ratio=neg_sampling_ratio,\n",
    "#         add_negative_train_samples=add_negative_train_samples,\n",
    "#         split_labels=True\n",
    "#     )\n",
    "\n",
    "#     # Apply the transform\n",
    "#     train_data, _, test_data = transform(\n",
    "#         Data(\n",
    "#             edge_index=graph.edge_index, \n",
    "#             num_nodes=ecore_graph.number_of_nodes()\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "#     return train_data, test_data\n",
    "\n",
    "#     # train_graph_data = SEALGraphData(\n",
    "#     #     graph, \n",
    "#     #     train_data.edge_index, \n",
    "#     #     train_data.pos_edge_label_index, \n",
    "#     #     train_data.neg_edge_label_index,\n",
    "#     #     hops=hops\n",
    "    \n",
    "#     # )\n",
    "\n",
    "#     # test_graph_data = SEALGraphData(\n",
    "#     #     graph, \n",
    "#     #     test_data.edge_index, \n",
    "#     #     test_data.pos_edge_label_index, \n",
    "#     #     test_data.neg_edge_label_index,\n",
    "#     #     hops=hops\n",
    "#     # )\n",
    "\n",
    "#     # return train_graph_data, test_graph_data\n",
    "    \n",
    "\n",
    "# train_data, test_data = get_link_prediction_train_test_graph_data(ecore_graph)\n",
    "# # Create train and test dataloaders\n",
    "# batch_size = 32\n",
    "\n",
    "# # train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# # test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_embeddings(graph, embedder: BertEmbedder, hops=1):\n",
    "    \"\"\"\n",
    "    Embed nodes in a graph using a given embedder.\n",
    "    \n",
    "    Parameters:\n",
    "    graph (networkx.Graph): The graph.\n",
    "    embedder (Embedder): The embedder.\n",
    "    hops (int): The number of hops.\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: The node embeddings.\n",
    "    \"\"\"\n",
    "    node_texts = get_node_texts(graph, hops)\n",
    "    node_texts = list(node_texts.values())\n",
    "    node_embeddings = embedder.embed(node_texts)\n",
    "    return node_embeddings\n",
    "\n",
    "\n",
    "def get_edge_embeddings(graph, embedder: BertEmbedder):\n",
    "    \"\"\"\n",
    "    Embed edges in a graph using a given embedder.\n",
    "    \n",
    "    Parameters:\n",
    "    graph (networkx.Graph): The graph.\n",
    "    embedder (Embedder): The embedder.\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: The edge embeddings.\n",
    "    \"\"\"\n",
    "    edge_texts = get_edge_texts(graph)\n",
    "    edge_texts = list(edge_texts.values())\n",
    "    edge_embeddings = embedder.embed(edge_texts)\n",
    "    return edge_embeddings\n",
    "\n",
    "def embed_graph(graph, embedder: BertEmbedder, hops=1):\n",
    "    \"\"\"\n",
    "    Embed a graph using a given embedder.\n",
    "    \n",
    "    Parameters:\n",
    "    graph (networkx.Graph): The graph.\n",
    "    embedder (Embedder): The embedder.\n",
    "    hops (int): The number of hops.\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: The graph embedding.\n",
    "    \"\"\"\n",
    "    node_embeddings = get_node_embeddings(graph, embedder, hops)\n",
    "    edge_embeddings = get_edge_embeddings(graph, embedder)\n",
    "    return node_embeddings, edge_embeddings\n",
    "\n",
    "node_embeddings, edge_embeddings = embed_graph(ecore_graph, embedder, hops=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "\n",
    "class GraphDataset(Dataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            graphs, \n",
    "            hops=1,\n",
    "        ):\n",
    "        self.graphs = graphs\n",
    "        self.hops = hops\n",
    "\n",
    "    def embed_graphs(self):\n",
    "        for graph in self.graphs:\n",
    "            yield graph.x\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.graphs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EcoreNxG' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m shuffle\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StratifiedKFold\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTorchGraph\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m      9\u001b[0m             \u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m     10\u001b[0m             graph: EcoreNxG, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m             neg_samples_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     16\u001b[0m         ):\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph \u001b[38;5;241m=\u001b[39m graph\n",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m, in \u001b[0;36mTorchGraph\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTorchGraph\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m      9\u001b[0m             \u001b[38;5;28mself\u001b[39m, \n\u001b[0;32m---> 10\u001b[0m             graph: \u001b[43mEcoreNxG\u001b[49m, \n\u001b[1;32m     11\u001b[0m             save_dir: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     12\u001b[0m             distance \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     13\u001b[0m             lptr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m     14\u001b[0m             use_neg_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     15\u001b[0m             neg_samples_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     16\u001b[0m         ):\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph \u001b[38;5;241m=\u001b[39m graph\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistance \u001b[38;5;241m=\u001b[39m distance\n",
      "\u001b[0;31mNameError\u001b[0m: name 'EcoreNxG' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "from random import shuffle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "class TorchGraph:\n",
    "    def __init__(\n",
    "            self, \n",
    "            graph: EcoreNxG, \n",
    "            save_dir: str,\n",
    "            distance = 1,\n",
    "            lptr=0.2,\n",
    "            use_neg_samples=False,\n",
    "            neg_samples_ratio=1,\n",
    "        ):\n",
    "        self.graph = graph\n",
    "        self.distance = distance\n",
    "        self.add_negative_train_samples = use_neg_samples\n",
    "        self.neg_sampling_ratio = neg_samples_ratio\n",
    "        self.lptr = lptr\n",
    "        self.save_dir = save_dir\n",
    "        self.process_graph()\n",
    "    \n",
    "\n",
    "    def process_graph(self):\n",
    "        if not self.load_pyg_data():\n",
    "            self.data = self.get_pyg_data()\n",
    "            self.validate_data()\n",
    "                    \n",
    "        self.save()\n",
    "\n",
    "    \n",
    "    def get_pyg_data(self):\n",
    "        transform = RandomLinkSplit(\n",
    "            num_val=0, \n",
    "            num_test=self.lptr, \n",
    "            add_negative_train_samples=self.add_negative_train_samples,\n",
    "            neg_sampling_ratio=self.neg_sampling_ratio\n",
    "        )\n",
    "\n",
    "        train_data, _, test_data = transform(Data(\n",
    "            edge_index=self.graph.edge_index, \n",
    "            num_nodes=self.graph.number_of_nodes()\n",
    "        ))\n",
    "        edge_index = train_data.edge_index\n",
    "        subgraph = self.graph.create_graph_from_edge_index(edge_index)\n",
    "        node_texts = self.graph.get_node_texts(subgraph, self.distance)\n",
    "        node_embeddings = embedder.embed(list(node_texts.values()))\n",
    "\n",
    "        edge_texts = self.graph.get_edge_texts(subgraph)\n",
    "        edge_embeddings = embedder.embed(list(edge_texts.values()))\n",
    "\n",
    "\n",
    "        data = Data(\n",
    "            x=node_embeddings,\n",
    "            edge_index=edge_index,\n",
    "            edge_attr=edge_embeddings,\n",
    "            train_data=train_data,\n",
    "            test_data=test_data,\n",
    "        )\n",
    "\n",
    "        return data\n",
    "    \n",
    "\n",
    "    def validate_data(self):\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return '.'.join(self.graph.graph_id.replace('/', '_').split('.')[:-1])\n",
    "\n",
    "\n",
    "    @property\n",
    "    def save_idx(self):\n",
    "        path = os.path.join(self.save_dir, f'{self.graph.id}')\n",
    "        if embedder.finetuned:\n",
    "            path = f'{path}_finetuned'\n",
    "        return path\n",
    "\n",
    "\n",
    "    def save_to_mapping(self):\n",
    "        graph_embedding_file_map = dict()\n",
    "        fp = f'{self.save_dir}/mapping.json'\n",
    "        if os.path.exists(fp):\n",
    "            graph_embedding_file_map = json.load(open(fp, 'r'))\n",
    "        else:\n",
    "            graph_embedding_file_map = dict()\n",
    "        \n",
    "        graph_embedding_file_map[self.name] = self.graph.id\n",
    "        json.dump(graph_embedding_file_map, open(fp, 'w'), indent=4)\n",
    "\n",
    "\n",
    "    def load_pyg_data(self):\n",
    "\n",
    "        if os.path.exists(self.save_idx):\n",
    "            self.save_to_mapping()\n",
    "            self.data = torch.load(f\"{self.save_idx}/data.pt\")\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "\n",
    "    def save(self):\n",
    "        os.makedirs(self.save_idx, exist_ok=True)\n",
    "        torch.save(self.data, f\"{self.save_idx}/data.pt\")\n",
    "        self.save_to_mapping()\n",
    "\n",
    "\n",
    "class GraphDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            models_dataset: ModelDataset,\n",
    "            save_dir='datasets/graph_data',\n",
    "            distance=1\n",
    "        ):\n",
    "        self.save_dir = f'{save_dir}/{models_dataset.name}'\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        self.graphs = [\n",
    "            TorchGraph(\n",
    "                g, \n",
    "                save_dir=self.save_dir,\n",
    "                distance=distance\n",
    "            ) \n",
    "            for g in tqdm(models_dataset, desc=f'Processing {models_dataset.name}')\n",
    "        ]\n",
    "\n",
    "        self._c = {label:j for j, label in enumerate({g.label for g in models_dataset})}\n",
    "        self.labels = torch.tensor([self._c[g.label] for g in models_dataset], dtype=torch.long)\n",
    "        self.num_classes = len(self._c)\n",
    "        self.num_features = self.graphs[0].data.x.shape[-1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        return self.graphs[index].data, self.labels[index]\n",
    "    \n",
    "    def get_train_test_split(self, tr=0.8):\n",
    "        n = len(self.graphs)\n",
    "        train_size = int(n * tr)\n",
    "        idx = list(range(n))\n",
    "        shuffle(idx)\n",
    "        train_idx = idx[:train_size]\n",
    "        test_idx = idx[train_size:]\n",
    "        return train_idx, test_idx\n",
    "    \n",
    "\n",
    "    def k_fold_split(self, k=10):\n",
    "        kfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "        n = len(self.graphs)\n",
    "        for train_idx, test_idx in kfold.split(np.zeros(n), np.zeros(n)):\n",
    "            yield train_idx, test_idx\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Data(edge_index=[2, 14], num_nodes=17, edge_label=[28], edge_label_index=[2, 28]),\n",
       " Data(edge_index=[2, 14], num_nodes=17, edge_label=[0], edge_label_index=[2, 0]),\n",
       " Data(edge_index=[2, 14], num_nodes=17, edge_label=[6], edge_label_index=[2, 6]))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_val_graphs(\n",
    "        graphs,\n",
    "        num_val=0,\n",
    "        num_test=0.2,\n",
    "        add_negative_train_samples=True,\n",
    "        neg_sampling_ratio=1\n",
    "    ):\n",
    "\n",
    "    # Apply RandomLinkSplit to each graph\n",
    "    split_graphs = []\n",
    "    for graph in graphs:\n",
    "        transform = RandomLinkSplit(\n",
    "            num_val=num_val, \n",
    "            num_test=num_test, \n",
    "            add_negative_train_samples=add_negative_train_samples,\n",
    "            neg_sampling_ratio=neg_sampling_ratio\n",
    "        )\n",
    "        train_data, val_data, test_data = transform(graph)\n",
    "        split_graphs.append((graph, train_data, val_data, test_data))\n",
    "\n",
    "    # Flatten the split_graphs list to get individual datasets\n",
    "    train_graphs = [(graph, train) for train, _, _ in split_graphs]\n",
    "    val_graphs = [(graph, val) for _, val, _ in split_graphs]\n",
    "    test_graphs = [(graph, test) for _, _, test in split_graphs]\n",
    "\n",
    "    train_dataset = GraphDataset(train_graphs)\n",
    "    val_dataset = GraphDataset(val_graphs)\n",
    "    test_dataset = GraphDataset(test_graphs)\n",
    "\n",
    "    return {\n",
    "        'train': train_dataset,\n",
    "        'val': val_dataset,\n",
    "        'test': test_dataset\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    x = []\n",
    "    edge_index = []\n",
    "    edge_label = []\n",
    "    edge_label_index = []\n",
    "    \n",
    "    offset = 0\n",
    "    for data in batch:\n",
    "        x.append(data.x)\n",
    "        edge_index.append(data.edge_index + offset)\n",
    "        edge_label.append(data.edge_label)\n",
    "        edge_label_index.append(data.edge_label_index + offset)\n",
    "        \n",
    "        offset += data.x.size(0)\n",
    "    \n",
    "    x = torch.cat(x, dim=0)\n",
    "    edge_index = torch.cat(edge_index, dim=1)\n",
    "    edge_label = torch.cat(edge_label, dim=0)\n",
    "    edge_label_index = torch.cat(edge_label_index, dim=1)\n",
    "    \n",
    "    return Data(x=x, edge_index=edge_index, edge_label=edge_label, edge_label_index=edge_label_index)\n",
    "\n",
    "\n",
    "def create_graph_dataloaders(data, batch_size=2):\n",
    "    train_loader = DataLoader(data['train'], batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "    val_loader = DataLoader(data['val'], batch_size=batch_size, collate_fn=collate_fn, shuffle=False)\n",
    "    test_loader = DataLoader(data['test'], batch_size=batch_size, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "    return {\n",
    "        'train': train_loader,\n",
    "        'val': val_loader,\n",
    "        'test': test_loader\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_heads):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=num_heads, dropout=0.6)\n",
    "        self.conv2 = GATConv(hidden_channels * num_heads, out_channels, heads=1, concat=False, dropout=0.6)\n",
    "\n",
    "        self.link_pred_head = torch.nn.Linear(hidden_channels * num_heads, 1)  # Link prediction head\n",
    "        self.edge_class_head = torch.nn.Linear(hidden_channels * num_heads, 3)  # Edge classification head\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = torch.nn.functional.dropout(x, p=0.6, training=self.training)\n",
    "        x = torch.nn.functional.elu(self.conv1(x, edge_index))\n",
    "        x = torch.nn.functional.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        row, col = edge_index\n",
    "        edge_features = torch.cat([x[row], x[col]], dim=1)  # [num_edges, hidden_channels*2]\n",
    "        \n",
    "        # Link prediction\n",
    "        link_pred = torch.sigmoid(self.link_pred_head(edge_features)).squeeze()  # [num_edges]\n",
    "        \n",
    "        # Edge classification\n",
    "        edge_class = self.edge_class_head(edge_features)  # [num_edges, num_edge_types]\n",
    "\n",
    "\n",
    "        return link_pred, edge_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = GAT(in_channels=1, hidden_channels=8, out_channels=8, num_heads=8).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "def train(data_loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        \n",
    "        pos_edge_index = batch.edge_label_index[:, batch.edge_label == 1]\n",
    "        neg_edge_index = batch.edge_label_index[:, batch.edge_label == 0]\n",
    "        \n",
    "        pos_pred = torch.sigmoid((out[pos_edge_index[0]] * out[pos_edge_index[1]]).sum(dim=1))\n",
    "        neg_pred = torch.sigmoid((out[neg_edge_index[0]] * out[neg_edge_index[1]]).sum(dim=1))\n",
    "        \n",
    "        pos_label = torch.ones(pos_pred.size(0), device=device)\n",
    "        neg_label = torch.zeros(neg_pred.size(0), device=device)\n",
    "        \n",
    "        loss = criterion(pos_pred, pos_label) + criterion(neg_pred, neg_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "\n",
    "def test(data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total_pos = 0\n",
    "    total_neg = 0\n",
    "    for batch in data_loader:\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        \n",
    "        pos_edge_index = batch.edge_label_index[:, batch.edge_label == 1]\n",
    "        neg_edge_index = batch.edge_label_index[:, batch.edge_label == 0]\n",
    "        \n",
    "        pos_pred = torch.sigmoid((out[pos_edge_index[0]] * out[pos_edge_index[1]]).sum(dim=1))\n",
    "        neg_pred = torch.sigmoid((out[neg_edge_index[0]] * out[neg_edge_index[1]]).sum(dim=1))\n",
    "        \n",
    "        pos_correct = (pos_pred > 0.5).sum().item()\n",
    "        neg_correct = (neg_pred <= 0.5).sum().item()\n",
    "        \n",
    "        correct += pos_correct + neg_correct\n",
    "        total_pos += pos_pred.size(0)\n",
    "        total_neg += neg_pred.size(0)\n",
    "    \n",
    "    return correct / (total_pos + total_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "## random data object\n",
    "\n",
    "data = Data(\n",
    "    x=torch.randn(5, 16),\n",
    "    edge_index=torch.tensor([[0, 1, 1, 2, 2, 3, 4], [1, 0, 2, 1, 3, 2, 2]]),\n",
    "    edge_label=torch.tensor([1, 0, 1, 0, 1, 0, 1]),\n",
    "    edge_label_index=torch.tensor([[0, 1, 2, 3, 4, 5, 6], [1, 0, 1, 0, 1, 0, 1]])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
