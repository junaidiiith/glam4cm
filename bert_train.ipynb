{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "\n",
    "datasets_dir = 'datasets'\n",
    "ecore_json_path = os.path.join(datasets_dir, 'ecore_555/ecore_555.jsonl')\n",
    "mar_json_path = os.path.join(datasets_dir, 'mar-ecore-github/ecore-github.jsonl')\n",
    "modelsets_uml_json_path = os.path.join(datasets_dir, 'modelset/uml.jsonl')\n",
    "modelsets_ecore_json_path = os.path.join(datasets_dir, 'modelset/ecore.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mar-ecore-github from pickle\n",
      "Loaded mar-ecore-github with 5389 graphs\n",
      "Loaded mar-ecore-github with 5389 graphs\n",
      "Graphs: 2997\n"
     ]
    }
   ],
   "source": [
    "from data_loading.data import ModelDataset\n",
    "\n",
    "config_params = dict(\n",
    "    timeout = 120,\n",
    "    min_enr = 1.2,\n",
    "    min_edges = 10\n",
    ")\n",
    "\n",
    "dataset_name = 'modelset'\n",
    "dataset_name = 'ecore_555'\n",
    "dataset_name = 'mar-ecore-github'\n",
    "# ecore = ModelDataset(dataset_name, reload=False, **config_params)\n",
    "modelset = ModelDataset(dataset_name, reload=False, remove_duplicates=True, **config_params)\n",
    "# mar = ModelDataset('mar-ecore-github', reload=True, **config_params)\n",
    "\n",
    "\n",
    "# datasets = {\n",
    "#     'ecore': ecore,\n",
    "#     'modelset': modelset,\n",
    "#     'mar': mar\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "569b598afa0943f1bdfb1e2ef856bc24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing modelset:   0%|          | 0/830 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from data_loading.graph_dataset import GraphDataset\n",
    "\n",
    "graph_data_params = dict(\n",
    "    distance=2,\n",
    "    reload=False,\n",
    "    add_negative_train_samples=True,\n",
    "    neg_sampling_ratio=1,\n",
    "    use_edge_types=False,\n",
    ")\n",
    "\n",
    "graph_dataset = GraphDataset(modelset, **graph_data_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "EDGE_START = '<edge_begin>'\n",
    "EDGE_END = '<edge_end>'\n",
    "NODE_SEP = '<node_sep>'\n",
    "NODE_PATH_SEP = '<node_path_sep>'\n",
    "\n",
    "def get_special_tokens():\n",
    "    return {\n",
    "        'additional_special_tokens': [EDGE_START, EDGE_END, NODE_SEP, NODE_PATH_SEP]\n",
    "    }\n",
    "\n",
    "\n",
    "def get_tokenizer(model_name, special_tokens, max_length):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    tokenizer.add_special_tokens(special_tokens)\n",
    "    tokenizer.model_max_length = max_length\n",
    "    return tokenizer\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "special_tokens = get_special_tokens()\n",
    "max_length = 512\n",
    "tokenizer = get_tokenizer(model_name, special_tokens, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import LP_TASK_LINK_PRED\n",
    "\n",
    "task_name = LP_TASK_LINK_PRED\n",
    "\n",
    "bert_dataset = graph_dataset.get_link_prediction_data(\n",
    "    tokenizer=tokenizer,\n",
    "    distance=2,\n",
    "    task_type=LP_TASK_LINK_PRED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample_dataset(dataset, oversampling_ratio=0.7):\n",
    "    \"\"\"\n",
    "    This function oversamples the classes that occur less frequently in the dataset.\n",
    "    The occurence of each class is counted and each class is oversampled 70% of the difference between the most common class and the class in question.\n",
    "    \"\"\"\n",
    "\n",
    "    class_occurences = dataset[:]['labels'].numpy()\n",
    "    unique_classes, counts = np.unique(class_occurences, return_counts=True)\n",
    "    max_count = counts.max()\n",
    "    indices_with_oversamples = []\n",
    "    for class_idx, count in zip(unique_classes, counts):\n",
    "        class_indices = np.where(class_occurences == class_idx)[0]\n",
    "        indices_with_oversamples.extend(class_indices)\n",
    "        oversample_count = int(oversampling_ratio * (max_count - count))\n",
    "        indices_with_oversamples.extend(np.random.choice(class_indices, oversample_count))\n",
    "    \n",
    "    return indices_with_oversamples\n",
    "\n",
    "ind_w_oversamples = oversample_dataset(bert_dataset['train'])\n",
    "bert_dataset['train'].inputs = bert_dataset['train'][ind_w_oversamples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({1: 44719, 0: 44628}), Counter({1: 10797, 0: 10785}))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(bert_dataset['train'][:]['labels'].tolist()), Counter(bert_dataset['test'][:]['labels'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(30526, 768)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    roc_auc_score, \n",
    "    f1_score, \n",
    "    precision_score, \n",
    "    recall_score\n",
    ")\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_metrics_multi_classification(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    logits = torch.tensor(pred.predictions)\n",
    "    probabilites = F.softmax(logits, dim=-1).numpy()\n",
    "    acc = (preds == labels).mean()\n",
    "    roc = roc_auc_score(labels, probabilites, multi_class='ovr')\n",
    "    f1_macro = f1_score(labels, preds, average='macro')\n",
    "    f1_micro = f1_score(labels, preds, average='micro')\n",
    "    precision = precision_score(labels, preds, average='macro')\n",
    "    recall = recall_score(labels, preds, average='macro')\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'roc_auc': roc,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_micro': f1_micro,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "def compute_metrics_binary_classification(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    logits = torch.tensor(pred.predictions)\n",
    "    probabilites = F.softmax(logits, dim=-1).numpy()\n",
    "    acc = (preds == labels).mean()\n",
    "    roc = roc_auc_score(labels, probabilites, multi_class='ovr')\n",
    "    f1_macro = f1_score(labels, preds, average='macro')\n",
    "    f1_micro = f1_score(labels, preds, average='micro')\n",
    "    precision = precision_score(labels, preds, average='macro')\n",
    "    recall = recall_score(labels, preds, average='macro')\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'roc_auc': roc,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_micro': f1_micro,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import os\n",
    "\n",
    "output_dir = os.path.join(\n",
    "    'results',\n",
    "    dataset_name,\n",
    "    task_name\n",
    ")\n",
    "\n",
    "logs_dir = os.path.join(\n",
    "    'logs',\n",
    "    dataset_name,\n",
    "    task_name\n",
    ")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=128,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=logs_dir,\n",
    "    logging_steps=1000,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=bert_dataset['train'],\n",
    "    eval_dataset=bert_dataset['test'],\n",
    "    compute_metrics=compute_metrics_binary_classification\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
